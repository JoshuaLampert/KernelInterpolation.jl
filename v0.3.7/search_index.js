var documenterSearchIndex = {"docs":
[{"location":"tutorial_noisy_data/#Dealing-with-noisy-data","page":"Dealing with noisy data","title":"Dealing with noisy data","text":"This tutorial is based on Chapter 19 of [Fasshauer2007] and will show how we can use regularization techniques and least-squares fitting to deal with noisy data. Most of the code for this tutorial is also available in the two examples interpolation/regularization_2d.jl and interpolation/least_squares_2d.jl.","category":"section"},{"location":"tutorial_noisy_data/#Define-problem-setup-and-perform-interpolation","page":"Dealing with noisy data","title":"Define problem setup and perform interpolation","text":"We start by defining a simple two-dimensional interpolation problem. We will use the famous Franke function as the target function and add some noise to the function values. The Franke function is defined as\n\nf(x y) = frac34expleft(-frac(9x - 2)^24 - frac(9y - 2)^24right) + frac34expleft(-frac(9x + 1)^249 - frac9y + 110right) + frac12expleft(-frac(9x - 7)^24 - frac(9y - 3)^24right) - frac15expleft(-(9x - 4)^2 - (9y - 7)^2right)\n\nAs nodes for the interpolation, we choose a random set of 1089 points in the unit square.\n\nusing KernelInterpolation\nusing Random # hide\nRandom.seed!(1234) # hide\n\nfunction f(x)\n    0.75 * exp(-0.25 * ((9 * x[1] - 2)^2 + (9 * x[2] - 2)^2)) +\n    0.75 * exp(-(9 * x[1] + 1)^2 / 49 - (9 * x[2] + 1) / 10) +\n    0.5 * exp(-0.25 * ((9 * x[1] - 7)^2 + (9 * x[2] - 3)^2)) -\n    0.2 * exp(-(9 * x[1] - 4)^2 - (9 * x[2] - 7)^2)\nend\n\nN = 1089\nnodeset = random_hypercube(N; dim = 2)\nvalues = f.(nodeset)\nvalues_noisy = values .+ 0.03 * randn(N)\n\nAs kernel, let's use the ThinPlateSplineKernel, which uses linear augmentation. We start by performing the interpolation based on the noisy data.\n\nkernel = ThinPlateSplineKernel{dim(nodeset)}()\nitp = interpolate(nodeset, values_noisy, kernel)\n\nWe plot the resulting interpolation and compare it to the original Franke function.\n\nusing Plots\np1 = surface(itp, colorbar = false)\np2 = surface(homogeneous_hypercube(40; dim = 2), f, colorbar = false)\n\nplot(p1, p2, layout = (1, 2))\nsavefig(\"interpolation_noisy.png\") # hide\nnothing # hide\n\n(Image: Interpolation of noisy function values)\n\nWe can see that the interpolation looks much rougher than the original Franke function. This is expected since we fit the noisy data too closely. Therefore, we would like to find a way how to stabilize the approximation and reduce the influence of the noise.","category":"section"},{"location":"tutorial_noisy_data/#Use-regularization-to-stabilize-the-approximation","page":"Dealing with noisy data","title":"Use regularization to stabilize the approximation","text":"The first possibility to stabilize the approximation is to use regularization. One of the simplest regularization techniques is the L2-regularization (or also known as ridge regression). One way to motivate the L2-regularization is to consider the interpolation problem as a minimization problem. We can write the interpolation problem as\n\nmin_c in mathbbR^N frac12c^TAc\n\nsubject to the constraint Ac = f, where A is the interpolation matrix and f the function values. This problem can be solved with the help of Lagrange multipliers and it turns out the solution simply is c = A^-1f as we already know. The idea of L2-regularization is to relax the condition Ac = f and instead of enforcing the equality, we penalize the deviation from the equality by adding the L2-norm of the difference. This leads to the minimization problem\n\nmin_c in mathbbR^N frac12c^TAc + frac12lambdaAc - f_2^2\n\nComputing the gradient of this expression with respect to c and setting it to zero, we obtain the regularized solution\n\nc = (A + lambda I)^-1f\n\nassuming the regularity and symmetry of the interpolation matrix A. The parameter lambda is a regularization parameter that controls the trade-off between the interpolation error and the regularization term. The larger lambda is, the more the interpolation is regularized, which leads to a smoother approximation. In practice, this means that we only change the interpolation matrix by adding a constant to the diagonal. Note that the polynomial augmentation is not affected by the regularization. In KernelInterpolation.jl, we can pass a regularizer to the interpolate function.\n\nλ = 0.01\nitp_reg = interpolate(nodeset, values_noisy, kernel, regularization = L2Regularization(λ))\n\nPlotting the regularized interpolation, we can see that the approximation is much smoother than the unregularized interpolation and thus much closer to the underlying target function.\n\nsurface(itp_reg, colorbar = false)\nsavefig(\"interpolation_noisy_regularized.png\") # hide\nnothing # hide\n\n(Image: Regularized interpolation of noisy function values)\n\nWe compare the stability of the regularized and unregularized interpolation by looking a the condition numbers of the two system matrices.\n\nusing LinearAlgebra\nA_itp = system_matrix(itp)\nA_itp_reg = system_matrix(itp_reg)\ncond(A_itp), cond(A_itp_reg)\n\nWe can see that the condition number is drastically reduced from around 1.5e8 to 1.5e4 by using regularization. This means that the regularized interpolation is much more stable and less sensitive to the noise in the data.","category":"section"},{"location":"tutorial_noisy_data/#Use-least-squares-approximation-to-fit-noisy-data","page":"Dealing with noisy data","title":"Use least-squares approximation to fit noisy data","text":"As an alternative to using regularization, we can also use least-squares fitting to approximate the noisy data. The idea of least-squares approximation is to use another set of nodes to construct the RBF basis than we use for the interpolation. This means we construct another NodeSet consisting of the centers for the basis functions, which is smaller than the nodeset we use for the interpolation. If the nodeset is given by X = x_1 ldots x_N and the centers are Xi = xi_1 ldots xi_M with M le N, we obtain a rectangular system matrix AinmathbbR^Ntimes M with A_ij = K(x_j xi_k) for j = 1 ldots N and k = 1 ldots M. The overdetermined system Ac = f can be solved by the least-squares method. Again, only the kernel matrix part is affected by the least-squares approximation and the polynomial augmentation is not changed. In KernelInterpolation.jl, we can pass centers to the interpolate function.\n\nM = 81\ncenters = random_hypercube(M; dim = 2)\nls = interpolate(centers, nodeset, values_noisy, kernel)\n\nWe plot the least-squares approximation and, again, see a better fit to the underlying target function.\n\nsurface(ls, colorbar = false)\nsavefig(\"interpolation_noisy_least_squares.png\") # hide\nnothing # hide\n\n(Image: Least squares approximation of noisy function values)\n\nFinally, we compare the error of the three methods to the true data without noise:\n\nvalues_itp = itp.(nodeset)\nvalues_itp_reg = itp_reg.(nodeset)\nvalues_ls = ls.(nodeset)\nnorm(values_itp .- values), norm(values_itp_reg .- values), norm(values_ls .- values)\n\nwhich confirms our findings above as the errors of the stabilized schemes are smaller than the error of the unregularized interpolation. Note that we did not put much effort in optimizing the regularization parameter or the number of centers for the least-squares approximation and that there is still room for improvement.\n\n[Fasshauer2007]: Fasshauer (2007): Meshfree Approximation Methods with Matlab, World Scientific, DOI: 10.1142/6437.","category":"section"},{"location":"license/#License","page":"License","title":"License","text":"MIT License\n\nCopyright (c) 2023-present Joshua Lampert <joshua.lampert@uni-hamburg.de> and contributors\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","category":"section"},{"location":"development/#Development","page":"Development","title":"Development","text":"If you have any suggestions or ideas for improvements or new features, we are pleased to accept and discuss issues or if you are willing to contribute, feel free to open a pull request, even if it is only fixing a typo or improving the docs.","category":"section"},{"location":"development/#Changing-KernelInterpolation.jl-and-running-it-locally","page":"Development","title":"Changing KernelInterpolation.jl and running it locally","text":"If you plan to edit KernelInterpolation.jl, you first need to clone a local copy of the repository, which can be done by using git. It is recommended that you create a project, e.g. call it run, inside the repository, where you can add packages that you use during executing and testing KernelInterpolation.jl, but are not needed by KernelInterpolation.jl. This way you can keep the Project.toml of the main repository clean. To do so, you can execute the following lines in a terminal:\n\ngit clone https://github.com/JoshuaLampert/KernelInterpolation.jl.git\ncd KernelInterpolation.jl\nmkdir run\ncd run\njulia --project=. -e 'using Pkg; Pkg.develop(PackageSpec(path=\"..\"))' # Install local KernelInterpolation.jl clone\njulia --project=. -e 'using Pkg; Pkg.add([\"Plots\", \"QuasiMonteCarlo\", \"Meshes\", \"OrdinaryDiffEqRosenbrock\", \"OrdinaryDiffEqNonlinearSolve\"])' # Install additional packages\n\nIf you use other packages for executing KernelInterpolation.jl, you can add them to the project in the run directory in an analogous way as above. To use the Julia project within run, be sure to start the Julia REPL by\n\njulia --project=.\n\nif already inside the the run directory or julia --project=run if in the main directory of the repo.","category":"section"},{"location":"development/#Preview-of-the-documentation","page":"Development","title":"Preview of the documentation","text":"If you want to build the documentation locally, you can run\n\njulia --project=docs -e 'using Pkg; Pkg.develop(PackageSpec(path=pwd())); Pkg.instantiate()'\n\nonce from the KernelInterpolation.jl main directory to tell Documenter.jl to build the documentation of your local clone. To build the documentation, run\n\njulia --project=docs --color=yes docs/make.jl\n\nThe resulting .html files can then be found in docs/build/ and you can look at them by opening them in a browser. For pull requests from the main repository (i.e. not from a fork), the documentation is automatically built and can be previewed under https://joshualampert.github.io/KernelInterpolation.jl/previews/PRXXX/ where XXX is the number of the pull request.","category":"section"},{"location":"changelog/#Changelog","page":"Changelog","title":"Changelog","text":"KernelInterpolation.jl follows the interpretation of semantic versioning (semver) used in the Julia ecosystem. Notable changes will be documented in this file for human readability.","category":"section"},{"location":"changelog/#Changes-in-the-v0.3-lifecycle","page":"Changelog","title":"Changes in the v0.3 lifecycle","text":"","category":"section"},{"location":"changelog/#Added","page":"Changelog","title":"Added","text":"Allow passing a factorization to interpolate (#130).","category":"section"},{"location":"changelog/#Changed","page":"Changelog","title":"Changed","text":"Define slicing for NodeSets and deprecate values_along_dim (#139).\nFix order of PolyharmonicSplineKernel to return an integer (#127).","category":"section"},{"location":"changelog/#Changes-when-updating-to-v0.3-from-v0.2.x","page":"Changelog","title":"Changes when updating to v0.3 from v0.2.x","text":"","category":"section"},{"location":"changelog/#Added-2","page":"Changelog","title":"Added","text":"Added a keyword argument factorization_method to interpolate, interpolation_matrix, and least_squares_matrix to allow for different factorization methods (#130).\nGeneral floating point support (#121).","category":"section"},{"location":"changelog/#Changed-2","page":"Changelog","title":"Changed","text":"The functions random_hypersphere and random_hypersphere_boundary not require a Tuple for the argument center. Before, e.g., a Vector was allowed (#121).\nThe element type of NodeSets will now always be converted to a floating point type, i.e., also when integer values are passed. This is more consistent for an interpolation framework makes many things easier. A similar approach is also used in the Meshes.jl/CoordRefSystems.jl ecosystem (#121).","category":"section"},{"location":"changelog/#Changes-in-the-v0.2-lifecycle","page":"Changelog","title":"Changes in the v0.2 lifecycle","text":"","category":"section"},{"location":"changelog/#Added-3","page":"Changelog","title":"Added","text":"Added support for general RNG in random_* functions (#106).\nAdded LagrangeBasis (#103).","category":"section"},{"location":"changelog/#Changed-3","page":"Changelog","title":"Changed","text":"Use OrdinaryDiffEqRosenbrock.jl instead of OrdinaryDiffEq.jl in the examples and documentation (#108).\nFix seriestype for 1D plots (#101).","category":"section"},{"location":"changelog/#Changes-when-updating-to-v0.2-from-v0.1.x","page":"Changelog","title":"Changes when updating to v0.2 from v0.1.x","text":"","category":"section"},{"location":"changelog/#Added-4","page":"Changelog","title":"Added","text":"Added tutorial on noisy data (#95).\nAdded L2 regularization (#94).\nAdded least squares approximation (#93, #97).","category":"section"},{"location":"changelog/#Changed-4","page":"Changelog","title":"Changed","text":"Add interface for general bases and add StandardBasis. This is breaking for least squares approximations because the order of centers and nodeset needs to be swapped in the interpolate function. Alternatively, use the new StandardBasis (#100).","category":"section"},{"location":"changelog/#Changes-in-the-v0.1-lifecycle","page":"Changelog","title":"Changes in the v0.1 lifecycle","text":"","category":"section"},{"location":"changelog/#Added-5","page":"Changelog","title":"Added","text":"Added tutorial to documentation (#66).\nAdded PartialDerivativeOperator (#65).\nAdded compactly supported Wu kernels (#64).\nAdded compatibility for PointSets  from Meshes.jl (#63).","category":"section"},{"location":"interpolation/#classical_interpolation","page":"Interpolation","title":"Classical interpolation","text":"Kernel methods are well-suited to interpolate given function values, which are known at scattered data points in any space dimension. In this tutorial, we will discuss the basics of kernel-based interpolation methods and how these can be implemented using KernelInterpolation.jl.","category":"section"},{"location":"interpolation/#Mathematical-background","page":"Interpolation","title":"Mathematical background","text":"The general aim of scattered data interpolation is to find a function sOmegatomathbbR, a so-called interpolant, approximating an unknown function fOmegatomathbbR mapping a (potentially high-dimensional) domain OmegasubsetmathbbR^d to the real numbers. We assume, we only know values f_i of the function f at some specific points x_iinOmega i = 1 ldots N. These values could, e.g., be the measurements of an experiment. The interpolation task is to determine a (continuous) interpolant s that exactly takes the values\n\nbeginequationlabeleqinterpolationconditions\n    s(x_i) = f_iquad forall iin1ldots N\nendequation\n\nbut is defined for any xinOmega. A common way to solve such problems is to restrict the space of possible interpolants to search in to be finite-dimensional (we take it N-dimensional to obtain a system with N conditions for N unknowns), i.e. we can find a finite set of basis function b_1 ldots b_N, such that we can express any function from the subspace of continuous functions as a linear combination of these basis functions\n\nbeginequationlabeleqlinearcombination\n    s(x) = sumlimits_j = 1^Nc_jb_j(x)\nendequation\n\nwhere c_i i = 1ldots N are the coefficients that determine s. To find an (the?) interpolant satisfying the above N conditions given by \\eqref{eq:interpolationconditions}, the coefficients c = (c_i)_i = 1ldotsNinmathbbR^N need to fulfill the following system of linear equations:\n\nA_Xc = f_X\n\nwhere f_X = (f_i)_i = 1ldotsNinmathbbR^N is the vector of function values and the components of the Vandermonde matrix A_X are given by (A_X)_ij = b_j(x_i). If the matrix A_X is regular, this gives a unique solution c and we found the unique interpolant s given by \\eqref{eq:linearcombination}.\n\nOne important question that remains is how to choose the basis functions b_j. A criterion for a good basis is the guarantee of producing a regular Vandermonde matrix for any set of nodes X = x_1ldots x_N. However, the well-known Mairhuber-Curtis theorem, e.g. Theorem 2.3 in [Wendland2004], states that there does not exist an N-dimensional subspace of the set of continuous functions such that the Vandermonde matrix is invertible for any set X = x_1ldots x_NsubsetOmega if OmegasubsetmathbbR^d contains an interior node and dge 2, Nge 2. This negative result suggests that the basis should be chosen data-dependent, i.e. the basis functions depend on the nodes x_i. One possibility to do so is to choose b_j = K(cdot x_j) for a kernel function K OmegatimesOmegatomathbbR. The Vandermonde matrix corresponding to a kernel is then given by the entries (A_X)_ij = K(x_i x_j). One criterion for this matrix being invertible is that it is symmetric and positive definite. Symmetry of A_X can be achieved by demanding K to be symmetric in its both entries, i.e. K(x_i x_j) = K(x_j x_i). A kernel is called positive definite if the corresponding matrix is positive definite. One very famous and fundamental kernel is the Gauß kernel, which is given by\n\nK(x y) = mathrme^-x - y_2^2\n\nIt can be shown that the Gauß kernel is positive definite. The Gauß kernel is a member of the most common class of kernel functions, namely radial basis functions. A translation-invariant kernel function is given by K(x y) = Phi(x - y), where PhiOmegatomathbbR^d depends only on one variable. A radial basis function kernel is a translation-invariant kernel, where Phi is given by Phi(x) = phi(x_2) for a univariate function phimathbbR_ge 0tomathbbR, which is sometimes called basic function [Fasshauer2007]. The Gauß kernel, e.g., is given by the basic function\n\nphi(r) = mathrme^-r^2\n\nMany radial symmetric kernels come with a parameter, the so-called shape parameter varepsilon, which can be used to control the \"flatness\" of the kernel. The shape parameter simply acts as a multiplicative factor to the norm, i.e. for a general radial-symmetric kernel we take K(x y) = phi(varepsilonx - y_2).\n\nThe completion of the linear space of functions that is spanned by the basis given a specific kernel and a domain Omega, mathcalH_K Omega = overlinetextspanK(cdot x) xinOmega, is called native space and is a (reproducing kernel) Hilbert space (RKHS), which comes with an inner product given by\n\nlangle f grangle_K = sumlimits_i = 1^Nsumlimits_j = 1^Mc_i^fc_j^gK(x_i xi_j)\n\nfor f ginmathcalH_K Omega having the representations\n\nf(x) = sumlimits_i = 1^Nc_i^fK(x x_i) quadtextandquad g(x) = sumlimits_j = 1^Mc_j^gK(x xi_j)\n\nThe corresponding norm inherited by the kernel scalar product is denoted as cdot_K = sqrtlanglecdotcdotrangle_K.\n\nOften, it can be useful to augment the linear combination of kernel basis functions with a linear combination of multivariate polynomials p_k, i.e. the interpolant takes the form\n\ns(x) = sumlimits_j = 1^Nc_jK(x x_j) + sumlimits_k = 1^Qd_kp_k(x)\n\nwhere p_k are a basis functions (usually monomials) of the Q = beginpmatrixm - 1 + ddendpmatrix-dimensional space of polynomials of degree m - 1. To obtain a complete system of equations, we need to enforce the constraints\n\nsumlimits_j = 1^Nc_jp_k(x_j) = 0 quadforall k = 1ldotsQ\n\nThe linear system now consists of a system-matrix that has 4 blocks:\n\nbeginpmatrixA_X  PP^T  0endpmatrixbeginpmatrixcdendpmatrix = beginpmatrixf_X0endpmatrix\n\nwhere the entries of PinmathbbR^Ntimes Q are given by P_jk = p_k(x_j). This strategy does not only guarantee to be able to reproduce polynomials exactly, but also leads to a larger class of possible kernels that can be taken for the interpolation because now it is not required anymore that the kernel is positive definite, but it suffices that the new system matrix beginpmatrixA_X  PP^T  0endpmatrix is regular. This leads to the notion of conditionally positive definite kernels of order m, which are kernel functions that produce an invertible system matrix provided that the interpolant is augmented by polynomials of order m (i.e. degree m - 1). It turns out that any positive definite kernel (i.e. conditionally positive definite of order 0) is also conditionally positive definite of any order mge 0. One popular class of conditionally positive definite kernels are the polyharmonic splines, which are built by the basic function\n\nphi_k(r) = begincases\n  r^k text if  k text odd\n  r^klogr text if  k text even\nendcases\n\nand are of order m = leftlceilfrack2rightrceil for odd k and m = frack2 + 1 for even k.","category":"section"},{"location":"interpolation/#Performing-an-interpolation","page":"Interpolation","title":"Performing an interpolation","text":"To perform an interpolation with KernelInterpolation.jl, we need three basic building blocks: the scattered nodes X = x_1ldots x_N (see also the previous tutorial), the function values at these nodes f_X, and a kernel.\n\nWe start by creating a set of 200 Halton points in a square bounded by (00 00) and (10 10):\n\nusing KernelInterpolation\nusing QuasiMonteCarlo: sample, HaltonSample\nnodes = NodeSet(sample(200, [0.0, 0.0], [1.0, 1.0], HaltonSample())')\n\nFor testing purposes, we sample the function values from a given function f (in reality, you normally do not know f of course). We pick the Franke function, which is a widely used test function.\n\nfunction f(x)\n    0.75 * exp(-0.25 * ((9 * x[1] - 2)^2 + (9 * x[2] - 2)^2)) +\n    0.75 * exp(-(9 * x[1] + 1)^2 / 49 - (9 * x[2] + 1) / 10) +\n    0.5 * exp(-0.25 * ((9 * x[1] - 7)^2 + (9 * x[2] - 3)^2)) -\n    0.2 * exp(-(9 * x[1] - 4)^2 - (9 * x[2] - 7)^2)\nend\n\nf_X = f.(nodes)\n\nWe can visualize the Franke function evaluated at the nodes by\n\nusing Plots\nplot(nodes, f_X, zcolor = f_X)\nsavefig(\"franke_function.png\") # hide\nnothing # hide\n\n(Image: Franke function)\n\nFinally, we pick a kernel function and create an Interpolation object by calling interpolate. Here, we choose a PolyharmonicSplineKernel of second order, i.e. k = 2 (also known as ThinPlateSplineKernel). The order of the polynomials will automatically be determined by the chosen kernel, but can also explicitly be passed as a fourth argument.\n\nkernel = ThinPlateSplineKernel{dim(nodes)}()\nitp = interpolate(nodes, f_X, kernel)\n\nThe returned object can be treated as a function and we can evaluate it at any d-dimensional point in space. To check that itp really interpolates the given data, we can call\n\nmaximum(abs.(itp.(nodes) - f_X))\n\nEvaluating the interpolant in a node that is not part of the training set can be done by\n\nx = [0.5, 0.5]\nabs.(itp(x) - f(x))\n\nCrucial for the stability of solving the linear system is the condition number of the system matrix. We can check it by\n\nusing LinearAlgebra: cond\ncond(system_matrix(itp))\n\nThe condition number should be as small as possible. If it is too large, the interpolation might be unstable. The condition number depends on the choice of the kernel and the nodes. If the condition number is too large, you might want to try a different kernel or a different set. Here, we have an order of magnitude of 10^5, which is acceptable.","category":"section"},{"location":"interpolation/#Visualizing-the-results","page":"Interpolation","title":"Visualizing the results","text":"To visualize the interpolation, we can use evaluate the interpolant on a grid of points and plot the result. We can use the function homogeneous_hypercube to create a grid of points in the unit square.\n\nN = 50\nnodes_grid = homogeneous_hypercube(N, (0.0, 0.0), (1.0, 1.0))\nx = unique(nodes_grid[:, 1])\ny = unique(nodes_grid[:, 2])\nz_itp = reshape(itp.(nodes_grid), (N, N))'\np1 = plot(x, y, z_itp, st = :heatmap, colorbar = :none, title = \"Interpolation\")\nz_true = reshape(f.(nodes_grid), (N, N))'\np2 = plot(x, y, z_true, st = :heatmap, colorbar = :none, title = \"True function\")\nplot(p1, p2, layout = (1, 2))\nsavefig(\"interpolation_franke.png\") # hide\nnothing # hide\n\n(Image: Interpolation of the Franke function)\n\nInstead of creating a grid of points with homogeneous_hypercube and manually reshaping the result, we can also directly plot the interpolant with plot(itp; x_min = 0.0, x_max = 1.0), which will automatically create a grid of points and plot the result.\n\nFor a publication-ready visualization, we can use ParaView to visualize the interpolant. We can save the values of the interpolant and the original function at the grid to a VTK file by\n\nvtk_save(\"interpolation_franke\", nodes_grid, itp, f, keys = [\"interpolant\", \"true\"])\nrm(\"interpolation_franke.vtu\") #clean up again # hide\n\nIn ParaView, e.g., you can now switch between the properties \"interpolant\" and \"true\" to plot the values of the interpolant and the Franke function, respectively. For looking at the scattered data, it might be helpful to change the representation to \"Point Gaussian\". Common filters in ParaView to visualize the interpolant are the Delaunay2D filter to create a surface plot or Warp by Scalar filter to create a 3D plot.","category":"section"},{"location":"interpolation/#Overview-of-kernels-and-adding-a-custom-kernel","page":"Interpolation","title":"Overview of kernels and adding a custom kernel","text":"In the previous example, we used the ThinPlateSplineKernel, which is a predefined kernel in KernelInterpolation.jl. There is a number of different kernels already defined, which can be used in an analogous way. For an overview of the existing radial-symmetric kernels, see the following table.\n\nKernel name Formula Order Smoothness\nGaussKernel phi(r) = mathrme^-r^2 0 C^infty\nMultiquadricKernel phi(r) = (1 + r^2)^beta beta  0 lceilbetarceil C^infty\nInverseMultiquadricKernel phi(r) = (1 + r^2)^-beta beta  0 0 C^infty\nPolyharmonicSplineKernel phi_k(r) = begincases r^k text if  k text odd r^klogr text if  k text even endcases kinmathbbN leftlceilfrack2rightrceil for odd k and frack2 + 1 for even k C^k - 1 for odd k and C^k for even k\nThinPlateSplineKernel phi(r) = r^2logr 2 C^2\nWendlandKernel phi_dk(r) = begincasesp_dk(r) text if  0le rle 10 text elseendcases d kinmathbbN for some polynomial p_dk 0 C^2k\nWuKernel phi_lk(r) = begincasesp_lk(r) text if  0le rle 10 text elseendcases l kinmathbbN for some polynomial p_lk 0 C^2(l - k)\nRadialCharacteristicKernel phi(r) = (1 - r)^beta_+ betage(d + 1)2 0 C^0\nMaternKernel phi_nu(r) = frac2^1 - nuGamma(nu)left(sqrt2nurright)^nu K_nuleft(sqrt2nurright) nu  0 0 C^2(lceilnurceil - 1)\nRieszKernel phi(r) = -r^beta 0  beta  2 1 C^infty\n\nKernels can be composed by using SumKernel and ProductKernel. Anisotropic kernels can be created by using TransformationKernel, which applies a transformation to the input before evaluating the kernel.\n\nHowever, you can also define your own kernel. A radial-symmetric kernel is a subtype of KernelInterpolation.RadialSymmetricKernel, which in turn is a subtype of KernelInterpolation.AbstractKernel and needs to implement the functions phi and order. Let's define an exponential kernel with phi(r) = mathrme^-r^15 and use it for the interpolation problem above.\n\nstruct MyKernel{Dim} <: KernelInterpolation.RadialSymmetricKernel{Dim} end\nKernelInterpolation.phi(::MyKernel, r) = exp(-r^1.5)\nKernelInterpolation.order(::MyKernel) = 0\n\nkernel = MyKernel{2}()\nitp_quadratic = interpolate(nodes, f_X, kernel)\np3 = plot(itp_quadratic; x_min = 0.0, x_max = 1.0, title = \"Interpolation with custom kernel\", colorbar = :none)\nplot(p3, p2, layout = (1, 2))\nsavefig(\"interpolation_franke_custom.png\") # hide\nnothing # hide\n\n(Image: Interpolation with the custom kernel)\n\nKernels can be visualized by either plotting the kernel itself or together with a NodeSet to plot the multivariate form of it, or by plotting the kernel with a one-dimensional vector to plot the basic function:\n\nx = -2.0:0.01:2.0\np_kernel = plot(x, kernel; label = \"Custom kernel\")\nplot!(p_kernel, x, GaussKernel{2}(); label = \"Gauss kernel\", title = \"Basic function\")\np_nodes = plot(kernel; title = \"Bivariate custom kernel\")\nplot(p_kernel, p_nodes, layout = (1, 2))\nsavefig(\"kernel_custom.png\") # hide\nnothing # hide\n\n(Image: Custom kernel)","category":"section"},{"location":"interpolation/#Next-steps","page":"Interpolation","title":"Next steps","text":"More examples regarding interpolation can be found in the examples folder of the repository. Some easy possible tasks for further investigation are to try different kernels, more complicated sets of nodes, or different functions. When you try out to use more complex node sets, you might want to use the merge function to combine different NodeSets. Note, however, that the interpolation problem might become ill-posed if the nodes are too close to each other. If there are at least twice the same nodes, the system matrix will become singular. In this case, you might want to filter out the duplicate nodes, e.g., by using unique!. For more information on kernel-based interpolation methods, we recommend the following references:","category":"section"},{"location":"interpolation/#References","page":"Interpolation","title":"References","text":"[Wendland2004]: Wendland (2004): Scattered Data Approximation, Cambridge University Press, DOI: 10.1017/CBO9780511617539.\n\n[Fasshauer2007]: Fasshauer (2007): Meshfree Approximation Methods with Matlab, World Scientific, DOI: 10.1142/6437.","category":"section"},{"location":"tutorial_differentiating_interpolation/#One-dimensional-interpolation-and-differentiation","page":"1D interpolation and differentiation","title":"One-dimensional interpolation and differentiation","text":"In this tutorial, we will create a simple one-dimensional interpolation, investigate how to tune the interpolation method, and show how to apply differential operators on the resulting Interpolation object.","category":"section"},{"location":"tutorial_differentiating_interpolation/#Define-problem-setup-and-perform-interpolation","page":"1D interpolation and differentiation","title":"Define problem setup and perform interpolation","text":"We start by defining a simple one-dimensional interpolation problem. We will interpolate the oscillatory function\n\nf(x) = exp(sin(2x^2)) + 01(x - pi2)^2\n\nbetween x = -3 and x = 3. For simplicity, we take 25 equidistant points in the interval -3 3 as interpolation points.\n\nusing KernelInterpolation\nf(x) = exp(sin(2*x[1]^2)) + 0.1*(x[1] - pi/2)^2\nx_min = -3\nx_max = 3\nN = 25\nnodeset = NodeSet(LinRange(x_min, x_max, N))\nvalues = f.(nodeset)\n\nNext, we choose the kernel (radial basis function) for the interpolation. We use the Gaussian kernel with a fixed shape parameter of 0.5 and interpolate the function values.\n\nkernel = GaussKernel{1}(shape_parameter = 0.5)\nitp = interpolate(nodeset, values, kernel)\n\nLet's plot the interpolated function and the original function on a finer grid to see how well the interpolation works.\n\nusing Plots\nmany_nodes = NodeSet(LinRange(x_min, x_max, 200))\nplot(many_nodes, f, label = \"Original function\")\nscatter!(nodeset, f, label = \"Original data\", markershape = :star)\nplot!(many_nodes, itp, training_nodes = false, yrange = (0.0, 5.0))\nsavefig(\"interpolation_oscillatory.png\") # hide\nnothing # hide\n\n(Image: Interpolation of one-dimensional oscillatory function)\n\nUhh, that doesn't look too good. What happened?","category":"section"},{"location":"tutorial_differentiating_interpolation/#Finding-a-well-suited-interpolation-method","page":"1D interpolation and differentiation","title":"Finding a well-suited interpolation method","text":"We used the GaussKernel with a rather small shape parameter of 0.5, which leads to an ill-conditioned linear system of equations. We can inspect the condition number of the interpolation matrix to confirm this.\n\nusing LinearAlgebra\nA = system_matrix(itp)\ncond(A)\n\nHere, we used the system_matrix function to obtain the interpolation matrix A and calculated the condition number of the matrix. For this specific example the system matrix simply is the kernel_matrix, but for more sophisticated interpolations the system matrix contains additional parts like the polynomial augmentation. The condition number is a measure of how well-conditioned the matrix is. A large condition number indicates that the matrix is ill-conditioned, which usually leads to high numerical errors. To avoid this, we have different options. We can either increase the shape parameter of the kernel or we can use a different kernel. The GaussKernel is known to be rather ill-conditioned and other kernels like the WendlandKernel usually lead to better condition numbers. Here, we choose to increase the shape parameter of the Gaussian kernel to 1.8, which makes the interpolation more localized. Note, however, that you might need to choose another kernel if you increase the number of interpolation points.\n\nkernel = GaussKernel{1}(shape_parameter = 1.8)\nitp = interpolate(nodeset, values, kernel)\nplot(many_nodes, f, label = \"Original function\")\nplot!(many_nodes, itp, yrange = (0.0, 5.0))\nsavefig(\"interpolation_oscillatory_1_5.png\") # hide\nnothing # hide\n\n(Image: Interpolation of one-dimensional oscillatory function with higher shape parameter)\n\nWe can see a much better agreement between the original function and the interpolated function. We still observe some undershoots, but this is expected due to the oscillatory nature of the function and the limited number of interpolation points. Let's confirm that increasing the shape parameter improved the condition number of the interpolation matrix.\n\nA = system_matrix(itp)\ncond(A)\n\nIndeed, the condition number is much smaller than before!","category":"section"},{"location":"tutorial_differentiating_interpolation/#Applying-differential-operators","page":"1D interpolation and differentiation","title":"Applying differential operators","text":"Sometimes, we are not only interested in interpolating a function, but also in computing its derivatives. Remember that in the simplest case, where no polynomial augmentation is used, the interpolation itp represents a linear combination\n\ns(x) = sum_j = 1^N c_jphi(x - x_j_2)\n\nwith phi given by the radial basis function, in this case the Gaussian. Because we know phi and its derivatives, we can compute the derivatives of s by differentiating the kernel function. For a general dimension d, the partial derivative in the i-th direction, iin1ldotsd, of the interpolation is then given by\n\nfracpartial spartial x_i(x) = sum_j = 1^N c_jfracpartial phipartial x_i(x - x_j_2)\n\nnote: Note\nAlthough the derivatives of the kernel functions could be computed analytically, KernelInterpolation.jl uses automatic differentiation (AD) by using ForwardDiff.jl. This allows for flexibility, simplicity, and easier extension, but it might be slower than computing the derivatives analytically.\n\nKernelInterpolation.jl already provides some common differential operators. For example, we can compute the first derivative of the interpolation itp at a specific point x by using the PartialDerivative operator.\n\nd1 = PartialDerivative(1)\nx = 0.0\nitp_dx = d1(itp, x)\n\nLet's plot the first derivative of the interpolated function and compare it to the analytical first derivative.\n\nitp_dx_many_nodes = d1.(Ref(itp), many_nodes)\nf_dx(x) = 4*exp(sin(2*x[1]^2))*x[1]*cos(2*x[1]^2) + 0.2*x[1] - pi/10\nplot(many_nodes, f_dx, label = \"Derivative of original function\")\nplot!(many_nodes, itp_dx_many_nodes, label = \"Derivative of interpolated function\")","category":"section"},{"location":"ref/#KernelInterpolation.jl-API","page":"Reference","title":"KernelInterpolation.jl API","text":"","category":"section"},{"location":"ref/#api-kernels","page":"Reference","title":"Kernel functions","text":"","category":"section"},{"location":"ref/#Node-sets","page":"Reference","title":"Node sets","text":"","category":"section"},{"location":"ref/#Bases","page":"Reference","title":"Bases","text":"","category":"section"},{"location":"ref/#Interpolation","page":"Reference","title":"Interpolation","text":"","category":"section"},{"location":"ref/#Regularization","page":"Reference","title":"Regularization","text":"","category":"section"},{"location":"ref/#api-diffops","page":"Reference","title":"Differential Operators","text":"","category":"section"},{"location":"ref/#api-equations","page":"Reference","title":"Partial differential equations","text":"","category":"section"},{"location":"ref/#Discretization","page":"Reference","title":"Discretization","text":"","category":"section"},{"location":"ref/#Kernel-matrices","page":"Reference","title":"Kernel matrices","text":"","category":"section"},{"location":"ref/#api-callbacks","page":"Reference","title":"Callbacks","text":"","category":"section"},{"location":"ref/#Input/Output","page":"Reference","title":"Input/Output","text":"","category":"section"},{"location":"ref/#Utilities","page":"Reference","title":"Utilities","text":"","category":"section"},{"location":"ref/#KernelInterpolation.KernelInterpolation","page":"Reference","title":"KernelInterpolation.KernelInterpolation","text":"KernelInterpolation\n\nKernelInterpolation.jl is a Julia package that implements methods for multivariate interpolation in arbitrary dimension based on symmetric (conditionally) positive-definite kernels with a focus on radial basis functions. It can be used for classical interpolation of scattered data, as well as for generalized (Hermite-Birkhoff) interpolation by using a meshfree collocation approach. This can be used to solve partial differential equations both stationary ones and time-dependent ones by using some time integration method from OrdinaryDiffEq.jl.\n\nSee also: KernelInterpolation.jl\n\n\n\n\n\n","category":"module"},{"location":"ref/#KernelInterpolation.AbstractKernel","page":"Reference","title":"KernelInterpolation.AbstractKernel","text":"AbstractKernel\n\nAn abstract supertype of kernels.\n\n\n\n\n\n","category":"type"},{"location":"ref/#KernelInterpolation.dim-Union{Tuple{KernelInterpolation.AbstractKernel{Dim}}, Tuple{Dim}} where Dim","page":"Reference","title":"KernelInterpolation.dim","text":"dim(kernel)\n\nReturn the dimension of a kernel, i.e. the size of the input vector.\n\n\n\n\n\n","category":"method"},{"location":"ref/#KernelInterpolation.get_name-Tuple{KernelInterpolation.AbstractKernel}","page":"Reference","title":"KernelInterpolation.get_name","text":"get_name(kernel::AbstractKernel)\n\nReturns the canonical, human-readable name for the given system of equations.\n\n\n\n\n\n","category":"method"},{"location":"ref/#KernelInterpolation.GaussKernel","page":"Reference","title":"KernelInterpolation.GaussKernel","text":"GaussKernel{Dim}(; shape_parameter = 1.0)\n\nGaussian kernel function with\n\n    phi(r) = exp(-(varepsilon r)^2)\n\nwhere varepsilon is the shape parameter. The Gaussian kernel is always positive definite. See Wendland (2004), p. 74.\n\nSee also RadialSymmetricKernel.\n\nHolger Wendland (2004) Scattered Data Approximation Cambridge University Press DOI: 10.1017/CBO9780511617539\n\n\n\n\n\n","category":"type"},{"location":"ref/#KernelInterpolation.InverseMultiquadricKernel","page":"Reference","title":"KernelInterpolation.InverseMultiquadricKernel","text":"InverseMultiquadricKernel{Dim}(beta = 0.5; shape_parameter = 1.0)\n\nInverse multiquadric kernel function with\n\n    phi(r) = (1 + (varepsilon r)^2)^-beta\n\nwhere varepsilon is the shape parameter. The inverse multiquadric kernel is always positive definite. See Wendland (2004), p. 76 and p. 95.\n\nSee also RadialSymmetricKernel.\n\nHolger Wendland (2004) Scattered Data Approximation Cambridge University Press DOI: 10.1017/CBO9780511617539\n\n\n\n\n\n","category":"type"},{"location":"ref/#KernelInterpolation.Matern12Kernel","page":"Reference","title":"KernelInterpolation.Matern12Kernel","text":"Matern12Kernel{Dim}(; shape_parameter = 1.0)\n\nMatern kernel with nu = 12, i.e.,\n\n    phi(r) = exp(-varepsilon r)\n\nwhere varepsilon is the shape parameter. The Matern kernel is positive definite.\n\nSee Wikipedia and Fasshauer (2007), p. 41.\n\nSee also MaternKernel, RadialSymmetricKernel.\n\nGregory Fasshauer (2007) Meshfree Approximation Methods with MATLAB World Scientific DOI: 10.1142/6437\n\n\n\n\n\n","category":"type"},{"location":"ref/#KernelInterpolation.Matern32Kernel","page":"Reference","title":"KernelInterpolation.Matern32Kernel","text":"Matern32Kernel{Dim}(; shape_parameter = 1.0)\n\nMatern kernel with nu = 32, i.e.,\n\n    phi(r) =  (1 + sqrt3varepsilon r)exp(-sqrt3varepsilon r)\n\nwhere varepsilon is the shape parameter. The Matern kernel is positive definite.\n\nSee Wikipedia and Fasshauer (2007), p. 41.\n\nSee also MaternKernel, RadialSymmetricKernel.\n\nGregory Fasshauer (2007) Meshfree Approximation Methods with MATLAB World Scientific DOI: 10.1142/6437\n\n\n\n\n\n","category":"type"},{"location":"ref/#KernelInterpolation.Matern52Kernel","page":"Reference","title":"KernelInterpolation.Matern52Kernel","text":"Matern52Kernel{Dim}(; shape_parameter = 1.0)\n\nMatern kernel with nu = 52, i.e.,\n\n    phi(r) =  (1 + sqrt5varepsilon r + 5cdot(varepsilon r)^23)exp(-sqrt5varepsilon r)\n\nwhere varepsilon is the shape parameter. The Matern kernel is positive definite.\n\nSee Wikipedia and Fasshauer (2007), p. 41.\n\nSee also MaternKernel, RadialSymmetricKernel.\n\nGregory Fasshauer (2007) Meshfree Approximation Methods with MATLAB World Scientific DOI: 10.1142/6437\n\n\n\n\n\n","category":"type"},{"location":"ref/#KernelInterpolation.Matern72Kernel","page":"Reference","title":"KernelInterpolation.Matern72Kernel","text":"Matern72Kernel{Dim}(; shape_parameter = 1.0)\n\nMatern kernel with nu = 72, i.e.,\n\n    phi(r) =  (1 + sqrt7varepsilon r + 12cdot(varepsilon r)^25 + 7cdot(varepsilon r)^315)exp(-sqrt7varepsilon r)\n\nwhere varepsilon is the shape parameter. The Matern kernel is positive definite.\n\nSee Wikipedia and Fasshauer (2007), p. 41.\n\nSee also MaternKernel, RadialSymmetricKernel.\n\nGregory Fasshauer (2007) Meshfree Approximation Methods with MATLAB World Scientific DOI: 10.1142/6437\n\n\n\n\n\n","category":"type"},{"location":"ref/#KernelInterpolation.MaternKernel","page":"Reference","title":"KernelInterpolation.MaternKernel","text":"MaternKernel{Dim}(nu = 1.5; shape_parameter = 1.0)\n\nMatern kernel with\n\n    phi_nu(r) =  frac2^1-nuGamma(nu)big(sqrt2nuvarepsilon rbig)^nu K_nubig(sqrt2nuvarepsilon rbig)\n\nwhere varepsilon is the shape parameter. The Matern kernel is positive definite.\n\nSee Wikipedia and Fasshauer (2007), p. 41.\n\nSee also RadialSymmetricKernel.\n\nGregory Fasshauer (2007) Meshfree Approximation Methods with MATLAB World Scientific DOI: 10.1142/6437\n\n\n\n\n\n","category":"type"},{"location":"ref/#KernelInterpolation.MultiquadricKernel","page":"Reference","title":"KernelInterpolation.MultiquadricKernel","text":"MultiquadricKernel{Dim}(beta = 0.5; shape_parameter = 1.0)\n\nMultiquadric kernel function with\n\n    phi(r) = (1 + (varepsilon r)^2)^beta\n\nwhere varepsilon is the shape parameter. The multiquadric kernel is conditionally positive definite of order m = lceilbeta rceil. See Wendland (2004), p. 109.\n\nSee also RadialSymmetricKernel.\n\nHolger Wendland (2004) Scattered Data Approximation Cambridge University Press DOI: 10.1017/CBO9780511617539\n\n\n\n\n\n","category":"type"},{"location":"ref/#KernelInterpolation.PolyharmonicSplineKernel","page":"Reference","title":"KernelInterpolation.PolyharmonicSplineKernel","text":"PolyharmonicSplineKernel{Dim}(k)\n\nPolyharmonic spline kernel function with\n\n    phi_k(r) = begincases\n        r^k text if  k text odd\n        r^klog(r) text if  k text even\n    endcases\n\nThe polyharmonic spline is conditionally positive definite of order m = lceil k2rceil for odd k and order m = k2 + 1 for even k. See Wendland (2004), pp. 111–112.\n\nSee also RadialSymmetricKernel.\n\nHolger Wendland (2004) Scattered Data Approximation Cambridge University Press DOI: 10.1017/CBO9780511617539\n\n\n\n\n\n","category":"type"},{"location":"ref/#KernelInterpolation.RadialCharacteristicKernel","page":"Reference","title":"KernelInterpolation.RadialCharacteristicKernel","text":"RadialCharacteristicKernel{Dim}(beta = 2.0; shape_parameter = 1.0)\n\nRadial characteristic function (or also called truncated power or Askey) kernel function with\n\n    phi(r) = (1 - varepsilon r)^beta_+\n\nwhere varepsilon is the shape parameter. The radial characteristic function is positive definite if betage (d + 1)2. It is compactly supported. See Wendland (2004), p. 80, Iske (2018), p. 281.\n\nSee also RadialSymmetricKernel.\n\nHolger Wendland (2004) Scattered Data Approximation Cambridge University Press DOI: 10.1017/CBO9780511617539\nArmin Iske (2018) Approximation Theory and Algorithms for Data Analysis Texts in Applied Mathematics (Springer) DOI: 10.1007/978-3-030-05228-7\n\n\n\n\n\n","category":"type"},{"location":"ref/#KernelInterpolation.RadialSymmetricKernel","page":"Reference","title":"KernelInterpolation.RadialSymmetricKernel","text":"RadialSymmetricKernel\n\nAn abstract supertype of radial symmetric kernels. Radial symmetric kernels are generated by an even and continuous function Phi mathbbR^dtomathbbR, which is radial-symmetric meaning that there exists a phi0inftytomathbbR such that\n\n    Phi(x) = phi(Vert xVert)\n\nThe kernel is then defined by\n\n    K(x y) = Phi(x - y)\n\nA RadialSymmetricKernel can be evaluated at two points x and y by calling kernel(x, y) or at a single point x by calling kernel(x), which implicitly sets y to zero.\n\n\n\n\n\n","category":"type"},{"location":"ref/#KernelInterpolation.RieszKernel","page":"Reference","title":"KernelInterpolation.RieszKernel","text":"RieszKernel{Dim}(beta; shape_parameter = 1.0)\n\nRiesz kernel with\n\n    phi(r) =  -(varepsilon r)^beta\n\nwhere varepsilon is the shape parameter and betain (02). The Riesz kernel is conditionally positive definite of order 1. See Hertrich et al. (2023).\n\nSee also RadialSymmetricKernel.\n\nJohannes Hertrich, Christian Wald, Fabian Altekrüger, Paul Hagemann (2023) Generative Sliced MMD Flows with Riesz Kernels ArXiv: 2305.11463\n\n\n\n\n\n","category":"type"},{"location":"ref/#KernelInterpolation.ThinPlateSplineKernel","page":"Reference","title":"KernelInterpolation.ThinPlateSplineKernel","text":"ThinPlateSplineKernel{Dim}()\n\nThin plate spline kernel function with\n\n    phi(r) = r^2log(r)\n\ni.e., PolyharmonicSplineKernel with k = 2. The thin plate spline is conditionally positive definite of order m = 2. See Wendland (2004), p. 112.\n\nSee also RadialSymmetricKernel.\n\nHolger Wendland (2004) Scattered Data Approximation Cambridge University Press DOI: 10.1017/CBO9780511617539\n\n\n\n\n\n","category":"type"},{"location":"ref/#KernelInterpolation.WendlandKernel","page":"Reference","title":"KernelInterpolation.WendlandKernel","text":"WendlandKernel{Dim}(k; shape_parameter = 1.0, d = Dim)\n\nWendland kernel with\n\n    phi_dk(r) = begincases\n        p_dk(varepsilon r) text if  0le varepsilon rle 1\n        0 text if  varepsilon r  1\n    endcases\n\nwhere varepsilon is the shape parameter and p is a polynomial with minimal degree. The Wendland kernel is positive definite for d\\le Dim and compactly supported. See Wendland (2004), p. 129 or Fasshauer (2007), pp. 87–88.\n\nSee also RadialSymmetricKernel.\n\nHolger Wendland (2004) Scattered Data Approximation Cambridge University Press DOI: 10.1017/CBO9780511617539\nGregory Fasshauer (2007) Meshfree Approximation Methods with MATLAB World Scientific DOI: 10.1142/6437\n\n\n\n\n\n","category":"type"},{"location":"ref/#KernelInterpolation.WuKernel","page":"Reference","title":"KernelInterpolation.WuKernel","text":"WuKernel{Dim}(l, k; shape_parameter = 1.0)\n\nWu kernel with\n\n    phi_lk(r) = begincases\n        p_lk(varepsilon r) text if  0le varepsilon rle 1\n        0 text if  varepsilon r  1\n    endcases\n\nwhere varepsilon is the shape parameter, kle l, and p is a polynomial of degree 4l - 2k + 1. The Wu kernel is positive definite for Dimle 2k + 1 and compactly supported. See Fasshauer (2007), pp. 88–90 and Wu (1995).\n\nSee also RadialSymmetricKernel.\n\nGregory Fasshauer (2007) Meshfree Approximation Methods with MATLAB World Scientific DOI: 10.1142/6437\nZongmin Wu (1995) Compactly supported positive definite radial functions Advances in Computational Mathematics DOI: 10.1007/BF03177517\n\n\n\n\n\n","category":"type"},{"location":"ref/#KernelInterpolation.Phi-Union{Tuple{Dim}, Tuple{KernelInterpolation.RadialSymmetricKernel{Dim}, Any}} where Dim","page":"Reference","title":"KernelInterpolation.Phi","text":"Phi(kernel, x)\n\nFor a RadialSymmetricKernel kernel return value of the multivariate function Phi defined by Phi(x) = phi(Vert xVert).\n\nSee also RadialSymmetricKernel, phi.\n\n\n\n\n\n","category":"method"},{"location":"ref/#KernelInterpolation.order","page":"Reference","title":"KernelInterpolation.order","text":"order(kernel)\n\nReturn order of kernel.\n\n\n\n\n\n","category":"function"},{"location":"ref/#KernelInterpolation.phi","page":"Reference","title":"KernelInterpolation.phi","text":"phi(kernel, r)\n\nFor a RadialSymmetricKernel kernel return value of the univariate function phi defining the kernel.\n\nSee also RadialSymmetricKernel, Phi.\n\n\n\n\n\n","category":"function"},{"location":"ref/#KernelInterpolation.ProductKernel","page":"Reference","title":"KernelInterpolation.ProductKernel","text":"ProductKernel{Dim}(kernels)\n\nGiven a vector of kernels, construct a new kernel that multiplies the results of the component kernels, i.e., the new kernel K is given by\n\n    K(x y) = prod_i = 1^n K_i(x y)\n\nwhere K_i are the component kernels and n the number of kernels. Note that all component kernels need to have the same dim. A ProductKernel can also be constructed using the * operator.\n\n\n\n\n\n","category":"type"},{"location":"ref/#KernelInterpolation.SumKernel","page":"Reference","title":"KernelInterpolation.SumKernel","text":"SumKernel{Dim}(kernels)\n\nGiven a vector of kernels, construct a new kernel that sums the results of the component kernels, i.e., the new kernel K is given by\n\n    K(x y) = sum_i = 1^n K_i(x y)\n\nwhere K_i are the component kernels and n the number of kernels. Note that all component kernels need to have the same dim. A SumKernel can also be constructed using the + operator.\n\n\n\n\n\n","category":"type"},{"location":"ref/#KernelInterpolation.TransformationKernel","page":"Reference","title":"KernelInterpolation.TransformationKernel","text":"TransformationKernel{Dim}(kernel, transformation)\n\nGiven a base kernel and a bijective transformation function, construct a new kernel that applies the transformation to both arguments x and y, i.e., the new kernel K_T is given by\n\n    K_T(x y) = K(Tx Ty)\n\nwhere K is the base kernel and T the transformation, i.e. if K is a kernel of dimension d, T is a function from dimension Dim to d, where Dim is the dimension of the new kernel.\n\n\n\n\n\n","category":"type"},{"location":"ref/#KernelInterpolation.NodeSet","page":"Reference","title":"KernelInterpolation.NodeSet","text":"NodeSet(nodes)\n\nSet of interpolation nodes.\n\n\n\n\n\n","category":"type"},{"location":"ref/#KernelInterpolation.distance_matrix-Tuple{NodeSet, NodeSet}","page":"Reference","title":"KernelInterpolation.distance_matrix","text":"distance_matrix(nodeset1::NodeSet, nodeset2::NodeSet)\n\nCompute the distance matrix between two NodeSets, which is a matrix D with D_ij = x_i - xi_j for all i and j, where x_i are the nodes in nodeset1 and xi_j are the nodes on nodeset2.\n\n\n\n\n\n","category":"method"},{"location":"ref/#KernelInterpolation.empty_nodeset","page":"Reference","title":"KernelInterpolation.empty_nodeset","text":"empty_nodeset(Dim, RealT = Float64)\n\nCreate an empty NodeSet.\n\n\n\n\n\n","category":"function"},{"location":"ref/#KernelInterpolation.homogeneous_hypercube","page":"Reference","title":"KernelInterpolation.homogeneous_hypercube","text":"homogeneous_hypercube(n, x_min = ntuple(_ -> 0.0, dim), x_max = ntuple(_ -> 1.0, dim); [dim])\n\nIf n is integer, create a NodeSet with n homogeneously distributed nodes in every dimension each of dimension dim inside a hypercube defined by the bounds x_min and x_max. If n is a Tuple of length dim, then use as many nodes in each dimension as described by n. The resulting NodeSet will have n^textrmdim respectively prod_j = 1^textrmdimn_j points. If the bounds are given as single values, they are applied for each dimension. If they are Tuples of size dim, the hypercube has the according bounds. If dim is not given explicitly, it is inferred by the lengths of n, x_min and x_max if possible.\n\n\n\n\n\n","category":"function"},{"location":"ref/#KernelInterpolation.homogeneous_hypercube_boundary","page":"Reference","title":"KernelInterpolation.homogeneous_hypercube_boundary","text":"homogeneous_hypercube_boundary(n, x_min = ntuple(_ -> 0.0, dim), x_max = ntuple(_ -> 1.0, dim); [dim])\n\nIf n is integer, create a NodeSet with n homogeneously distributed nodes in every dimension each of dimension dim on the boundary of a hypercube defined by the bounds x_min and x_max. If n is a Tuple of length dim, then use as many nodes in each dimension as described by n. If the bounds are given as single values, they are applied for each dimension. If they are Tuples of size dim, the hypercube has the according bounds. If dim is not given explicitly, it is inferred by the lengths of n, x_min and x_max if possible.\n\n\n\n\n\n","category":"function"},{"location":"ref/#KernelInterpolation.random_hypercube","page":"Reference","title":"KernelInterpolation.random_hypercube","text":"random_hypercube([rng], n, x_min = ntuple(_ -> 0.0, dim), x_max = ntuple(_ -> 1.0, dim); [dim])\n\nCreate a NodeSet with n random nodes each of dimension dim inside a hypercube defined by the bounds x_min and x_max. If the bounds are given as single values, they are applied for each dimension. If they are Tuples of size dim the hypercube has the according bounds. If dim is not given explicitly, it is inferred by the lengths of x_min and x_max if possible. Optionally, pass a random number generator rng.\n\n\n\n\n\n","category":"function"},{"location":"ref/#KernelInterpolation.random_hypercube_boundary","page":"Reference","title":"KernelInterpolation.random_hypercube_boundary","text":"random_hypercube_boundary([rng], n, x_min = ntuple(_ -> 0.0, dim), x_max = ntuple(_ -> 1.0, dim); [dim])\n\nCreate a NodeSet with n random nodes each of dimension dim on the boundary of a hypercube defined by the bounds x_min and x_max. If the bounds are given as single values, they are applied for each dimension. If they are Tuples of size dim the hypercube has the according bounds. If dim is not given explicitly, it is inferred by the lengths of x_min and x_max if possible. Optionally, pass a random number generator rng.\n\n\n\n\n\n","category":"function"},{"location":"ref/#KernelInterpolation.random_hypersphere","page":"Reference","title":"KernelInterpolation.random_hypersphere","text":"random_hypersphere([rng], n, r = 1.0, center = Tuple(zeros(dim)); [dim])\n\nCreate a NodeSet with n random nodes each of dimension dim inside a hypersphere with radius r around the center center, which is given as a tuple. If dim is not given explicitly, it is inferred by the length of center if possible. Optionally, pass a random number generator rng.\n\n\n\n\n\n","category":"function"},{"location":"ref/#KernelInterpolation.random_hypersphere_boundary","page":"Reference","title":"KernelInterpolation.random_hypersphere_boundary","text":"random_hypersphere_boundary([rng], n, r = 1.0, center = Tuple(zeros(dim)); [dim])\n\nCreate a NodeSet with n random nodes each of dimension dim at the boundary of a hypersphere with radius r around the center center, which is given as a tuple. If dim is not given explicitly, it is inferred by the length of center if possible. Optionally, pass a random number generator rng.\n\n\n\n\n\n","category":"function"},{"location":"ref/#KernelInterpolation.separation_distance-Tuple{NodeSet}","page":"Reference","title":"KernelInterpolation.separation_distance","text":"separation_distance(nodeset::NodeSet)\n\nReturn the separation distance of a NodeSet X = x_1ldots x_n defined by\n\n    q_X = frac12min_x_ineq x_jx_i - x_j\n\n\n\n\n\n","category":"method"},{"location":"ref/#KernelInterpolation.AbstractBasis","page":"Reference","title":"KernelInterpolation.AbstractBasis","text":"AbstractBasis\n\nAbstract type for a basis of a kernel function space. Every basis represents a set of functions, which can be obtained by indexing the basis object. Every basis object holds a kernel function and a NodeSet of centers and potentially more fields depending on the concrete basis type.\n\n\n\n\n\n","category":"type"},{"location":"ref/#KernelInterpolation.LagrangeBasis","page":"Reference","title":"KernelInterpolation.LagrangeBasis","text":"LagrangeBasis(centers, kernel, m = order(kernel))\n\nThe Lagrange (or cardinal) basis with respect to a kernel and a NodeSet of centers. This basis already includes polynomial augmentation of degree m defaulting to order(kernel). The basis functions are given such that\n\n    b_j(x_i) = delta_ij\n\nwhich means that the kernel_matrix of this basis is the identity matrix making it suitable if multiple interpolations with the same centers of the basis and the same kernel, but with different right-hand sides or nodesets are performed. Since the basis already includes polynomials no additional polynomial augmentation is needed for interpolation with this basis.\n\n\n\n\n\n","category":"type"},{"location":"ref/#KernelInterpolation.StandardBasis","page":"Reference","title":"KernelInterpolation.StandardBasis","text":"StandardBasis(centers, kernel)\n\nThe standard basis for a function space defined by a kernel and a NodeSet of centers. The basis functions are given by\n\n    b_j(x) = K(x x_j)\n\nwhere K is the kernel and x_j are the nodes in centers.\n\n\n\n\n\n","category":"type"},{"location":"ref/#KernelInterpolation.centers-Tuple{KernelInterpolation.AbstractBasis}","page":"Reference","title":"KernelInterpolation.centers","text":"centers(basis)\n\nReturn the centers from a basis object.\n\n\n\n\n\n","category":"method"},{"location":"ref/#KernelInterpolation.interpolation_kernel-Tuple{KernelInterpolation.AbstractBasis}","page":"Reference","title":"KernelInterpolation.interpolation_kernel","text":"interpolation_kernel(basis)\n\nReturn the kernel from a basis.\n\n\n\n\n\n","category":"method"},{"location":"ref/#KernelInterpolation.order-Tuple{KernelInterpolation.AbstractBasis}","page":"Reference","title":"KernelInterpolation.order","text":"order(basis)\n\nReturn the order m of the polynomial, which is needed by this basis for the interpolation, i.e., the polynomial degree plus 1. If m = 0, no polynomial is added.\n\n\n\n\n\n","category":"method"},{"location":"ref/#KernelInterpolation.Interpolation","page":"Reference","title":"KernelInterpolation.Interpolation","text":"Interpolation\n\nInterpolation object that can be evaluated at a node and represents a kernel interpolation of the form\n\n    s(x) = sum_j = 1^N c_jb_j(x) + sum_k = 1^Q d_kp_k(x)\n\nwhere b_j are the basis functions and p_k is a basis of the Q-dimensional space of multivariate polynomials of order order. The additional conditions\n\n    sum_j = 1^N c_jp_k(x_j) = 0 quad k = 1ldots Q\n\nare enforced.\n\nSee also interpolate.\n\n\n\n\n\n","category":"type"},{"location":"ref/#KernelInterpolation.TemporalInterpolation","page":"Reference","title":"KernelInterpolation.TemporalInterpolation","text":"TemporalInterpolation(ode_sol::ODESolution)\n\nTemporal interpolation of an ODE solution. The result can be evaluated at a time t and a spatial point x. Evaluating the interpolation at a time t returns an Interpolation object that can be evaluated at a spatial point x.\n\n\n\n\n\n","category":"type"},{"location":"ref/#KernelInterpolation.basis-Tuple{Interpolation}","page":"Reference","title":"KernelInterpolation.basis","text":"basis(itp)\n\nReturn the basis from an interpolation object.\n\n\n\n\n\n","category":"method"},{"location":"ref/#KernelInterpolation.centers-Tuple{Interpolation}","page":"Reference","title":"KernelInterpolation.centers","text":"centers(itp::Interpolation)\n\nReturn the centers from the basis of an interpolation object.\n\n\n\n\n\n","category":"method"},{"location":"ref/#KernelInterpolation.coefficients-Tuple{Interpolation}","page":"Reference","title":"KernelInterpolation.coefficients","text":"coefficients(itp::Interpolation)\n\nObtain all the coefficients of the linear combination for the interpolant, i.e., both the coefficients for the kernel part and for the polynomial part.\n\nSee also kernel_coefficients and polynomial_coefficients.\n\n\n\n\n\n","category":"method"},{"location":"ref/#KernelInterpolation.dim-Union{Tuple{Interpolation{Basis, Dim}}, Tuple{Dim}, Tuple{Basis}} where {Basis, Dim}","page":"Reference","title":"KernelInterpolation.dim","text":"dim(itp::Interpolation)\n\nReturn the dimension of the input variables of the interpolation.\n\n\n\n\n\n","category":"method"},{"location":"ref/#KernelInterpolation.interpolate-Union{Tuple{RealT}, Tuple{Dim}, Tuple{KernelInterpolation.AbstractBasis, Vector{RealT}}, Tuple{KernelInterpolation.AbstractBasis, Vector{RealT}, NodeSet{Dim, RealT}}} where {Dim, RealT}","page":"Reference","title":"KernelInterpolation.interpolate","text":"interpolate(basis, values, nodeset = centers(basis); m = order(basis),\n            regularization = NoRegularization(), factorization_method = nothing)\ninterpolate(centers, [nodeset,] values, kernel = GaussKernel{dim(nodeset)}();\n            m = order(kernel), regularization = NoRegularization(),\n            factorization_method = nothing)\n\nInterpolate the values evaluated at the nodes in the nodeset to a function using the kernel kernel and polynomials up to a order m (i.e. degree - 1), i.e., determine the coefficients c_j and d_k in the expansion\n\n    s(x) = sum_j = 1^N c_jb_j(x) + sum_k = 1^Q d_kp_k(x)\n\nwhere b_j are the basis functions in the basis and s(x) the interpolant s(x_j) = f(x_j), where f(x_j) are given by values, x_j are the nodes in the nodeset, and p_k is a basis of the Q-dimensional space of multivariate polynomials with maximum degree of m - 1. If m = 0, no polynomial is added. The additional conditions\n\n    sum_j = 1^N c_jp_k(x_j) = 0 quad k = 1ldots Q = beginpmatrixm - 1 + ddendpmatrix\n\nare enforced. Returns an Interpolation object.\n\nIf nodeset is provided, the interpolant is a least squares approximation with a different set of nodes as the centers used for the basis. Otherwise, nodeset is set to centers(basis) or centers.\n\nA regularization can be applied to the kernel matrix using the regularization argument, cf. regularize!. In addition, the factorization_method can be specified to determine how the system matrix is factorized. By default, the system matrix is just wrapped as a Symmetric matrix for interpolation and no factorization is applied for a least squares solution, but you can, e.g., also explicitly use cholesky, lu, or qr factorization.\n\n\n\n\n\n","category":"method"},{"location":"ref/#KernelInterpolation.interpolation_kernel-Tuple{KernelInterpolation.AbstractInterpolation}","page":"Reference","title":"KernelInterpolation.interpolation_kernel","text":"interpolation_kernel(itp)\n\nReturn the kernel from an interpolation object.\n\n\n\n\n\n","category":"method"},{"location":"ref/#KernelInterpolation.kernel_coefficients-Tuple{Interpolation}","page":"Reference","title":"KernelInterpolation.kernel_coefficients","text":"kernel_coefficients(itp::Interpolation)\n\nObtain the coefficients of the kernel part of the linear combination for the interpolant.\n\nSee also coefficients and polynomial_coefficients.\n\n\n\n\n\n","category":"method"},{"location":"ref/#KernelInterpolation.kernel_inner_product-Tuple{Any, Any}","page":"Reference","title":"KernelInterpolation.kernel_inner_product","text":"kernel_inner_product(itp1, itp2)\n\nInner product of the native space for two interpolants itp1 and itp2 with the same kernel. The inner product is defined as\n\n    langle f grangle_K = sum_i = 1^Nsum_j = 1^Mc_i^fc_j^gK(x_i xi_j)\n\nfor the interpolants f(x) = sum_i = 1^Nc_i^fK(x x_i) and g(x) = sum_j = 1^Mc_j^gK(x xi_j).\n\nSee also kernel_norm.\n\n\n\n\n\n","category":"method"},{"location":"ref/#KernelInterpolation.kernel_norm-Tuple{Any}","page":"Reference","title":"KernelInterpolation.kernel_norm","text":"kernel_norm(itp)\n\nNorm of the native space defined by the kernel of the interpolant itp. The norm is defined as\n\n    f_K^2 = sum_ij=1^Nc_ic_jK(x_i x_j)\n\nfor the interpolant f(x) = sum_j = 1^nc_jK(x x_j).\n\nSee also kernel_inner_product.\n\n\n\n\n\n","category":"method"},{"location":"ref/#KernelInterpolation.nodeset-Tuple{KernelInterpolation.AbstractInterpolation}","page":"Reference","title":"KernelInterpolation.nodeset","text":"nodeset(itp)\n\nReturn the node set from an interpolation object.\n\n\n\n\n\n","category":"method"},{"location":"ref/#KernelInterpolation.order-Tuple{Interpolation}","page":"Reference","title":"KernelInterpolation.order","text":"order(itp)\n\nReturn the order m of the polynomial used for the interpolation, i.e., the polynomial degree plus 1. If m = 0, no polynomial is added.\n\n\n\n\n\n","category":"method"},{"location":"ref/#KernelInterpolation.polynomial_basis-Tuple{Interpolation}","page":"Reference","title":"KernelInterpolation.polynomial_basis","text":"polynomial_basis(itp::Interpolation)\n\nReturn a vector of the polynomial basis functions used for the interpolation.\n\nSee also polyvars.\n\n\n\n\n\n","category":"method"},{"location":"ref/#KernelInterpolation.polynomial_coefficients-Tuple{Interpolation}","page":"Reference","title":"KernelInterpolation.polynomial_coefficients","text":"polynomial_coefficients(itp::Interpolation)\n\nObtain the coefficients of the polynomial part of the linear combination for the interpolant.\n\nSee also coefficients and kernel_coefficients.\n\n\n\n\n\n","category":"method"},{"location":"ref/#KernelInterpolation.polyvars-Tuple{Interpolation}","page":"Reference","title":"KernelInterpolation.polyvars","text":"polyvars(itp::Interpolation)\n\nReturn a vector of the polynomial variables.\n\nSee also polynomial_basis.\n\n\n\n\n\n","category":"method"},{"location":"ref/#KernelInterpolation.system_matrix-Tuple{Interpolation}","page":"Reference","title":"KernelInterpolation.system_matrix","text":"system_matrix(itp::Interpolation)\n\nReturn the system matrix, i.e., the matrix A in the linear system\n\n    Ac = f\n\nwhere c are the coefficients of the kernel interpolant and f the vector of known values. The exact form of A differs depending on which method is used.\n\n\n\n\n\n","category":"method"},{"location":"ref/#KernelInterpolation.AbstractRegularization","page":"Reference","title":"KernelInterpolation.AbstractRegularization","text":"AbstractRegularization\n\nAn abstract supertype of regularizations. A regularization implements a function regularize! that takes a matrix and returns a regularized version of it.\n\n\n\n\n\n","category":"type"},{"location":"ref/#KernelInterpolation.L2Regularization","page":"Reference","title":"KernelInterpolation.L2Regularization","text":"L2Regularization(regularization_parameter::Real)\n\nA regularization that adds a multiple of the identity matrix to the input matrix.\n\n\n\n\n\n","category":"type"},{"location":"ref/#KernelInterpolation.NoRegularization","page":"Reference","title":"KernelInterpolation.NoRegularization","text":"NoRegularization()\n\nA regularization that does nothing.\n\n\n\n\n\n","category":"type"},{"location":"ref/#KernelInterpolation.regularize!","page":"Reference","title":"KernelInterpolation.regularize!","text":"regularize!(A, reg::AbstractRegularization)\n\nApply the regularization reg to the matrix A in place.\n\n\n\n\n\n","category":"function"},{"location":"ref/#KernelInterpolation.EllipticOperator","page":"Reference","title":"KernelInterpolation.EllipticOperator","text":"EllipticOperator(A, b, c)\n\nLinear second-order elliptic operator with matrix A(x)inmathbbR^dtimes d, vector b(x)inmathbbR^d, and scalar c(x). The operator is defined as\n\n    mathcalLu = -sum_ij = 1^d a_ij(x)partial_x_ix_j^2u + sum_i = 1^db_i(x)partial_x_iu + c(x)u\n\nA, b and c are space-dependent functions returning a matrix, a vector, and a scalar, respectively. The matrix A should be symmetric and positive definite for any input x. The operator can be called with a RadialSymmetricKernel and points x and y to evaluate the operator of the kernel at x - y. It can also be called with an Interpolation object and a point x to evaluate the elliptic operator of the interpolation at x. Note that this is only supported for the kernel part of the interpolation, i.e. the polynomial part, if existent, is ignored.\n\n\n\n\n\n","category":"type"},{"location":"ref/#KernelInterpolation.Gradient","page":"Reference","title":"KernelInterpolation.Gradient","text":"Gradient()\n\nThe gradient operator. It can be called with a RadialSymmetricKernel and points x and y to evaluate the gradient of the kernel at x - y. It can also be called with an Interpolation object and a point x to evaluate the gradient of the interpolation at x. Note that this is only supported for the kernel part of the interpolation, i.e. the polynomial part, if existent, is ignored.\n\n\n\n\n\n","category":"type"},{"location":"ref/#KernelInterpolation.Laplacian","page":"Reference","title":"KernelInterpolation.Laplacian","text":"Laplacian()\n\nThe Laplacian operator. It can be called with a RadialSymmetricKernel and points x and y to evaluate the Laplacian of the kernel at x - y. It can also be called with an Interpolation object and a point x to evaluate the Laplacian of the interpolation at x. Note that this is only supported for the kernel part of the interpolation, i.e. the polynomial part, if existent, is ignored.\n\n\n\n\n\n","category":"type"},{"location":"ref/#KernelInterpolation.PartialDerivative","page":"Reference","title":"KernelInterpolation.PartialDerivative","text":"PartialDerivative(i)\n\nPartial derivative operator with respect to the i-th component. The operator can be called with a RadialSymmetricKernel and points x and y to evaluate the derivative of the kernel at x - y. It can also be called with an Interpolation object and a point x to evaluate the first partial derivative of the interpolation at x in the i-th direction. Note that this is only supported for the kernel part of the interpolation, i.e. the polynomial part, if existent, is ignored.\n\n\n\n\n\n","category":"type"},{"location":"ref/#KernelInterpolation.AdvectionDiffusionEquation","page":"Reference","title":"KernelInterpolation.AdvectionDiffusionEquation","text":"AdvectionDiffusionEquation(diffusivity, advection_velocity, f)\n\nAdvection-diffusion equation with diffusivity diffusivity and advection velocity advection_velocity. The advection-diffusion equation is defined as\n\n    partial_t u + mathbfacdotnabla u = kappaDelta u + f\n\nwhere mathbfa is the advection velocity, kappa is the diffusivity, and f is the right-hand side, which can be a time- and space-dependent function or a vector.\n\n\n\n\n\n","category":"type"},{"location":"ref/#KernelInterpolation.AdvectionEquation","page":"Reference","title":"KernelInterpolation.AdvectionEquation","text":"AdvectionEquation(advection_velocity)\n\nAdvection equation with advection velocity advection_velocity. The advection equation is defined as\n\n    partial_t u + mathbfacdotnabla u = f\n\nwhere mathbfa is the advection velocity and f a source term.\n\n\n\n\n\n","category":"type"},{"location":"ref/#KernelInterpolation.EllipticEquation","page":"Reference","title":"KernelInterpolation.EllipticEquation","text":"EllipticEquation(A, b, c, f)\n\nLibear second-order elliptic equation with matrix A, vector b, and scalar c and right-hand side f. The elliptic equation is defined as\n\n    mathcalLu = -sum_ij = 1^d a_ij(x)partial_x_ix_j^2u + sum_i = 1^db_i(x)partial_x_iu + c(x)u = f\n\nwhere A, b and c are space-dependent functions returning a matrix, a vector, and a scalar, respectively.\n\nSee also EllipticOperator.\n\n\n\n\n\n","category":"type"},{"location":"ref/#KernelInterpolation.HeatEquation","page":"Reference","title":"KernelInterpolation.HeatEquation","text":"HeatEquation(diffusivity, f)\n\nHeat equation with thermal diffusivity diffusivity. The heat equation is defined as\n\n    partial_t u = kappaDelta u + f\n\nwhere kappa is the thermal diffusivity and f is the right-hand side, which can be a time- and space-dependent function or a vector.\n\n\n\n\n\n","category":"type"},{"location":"ref/#KernelInterpolation.PoissonEquation","page":"Reference","title":"KernelInterpolation.PoissonEquation","text":"PoissonEquation(f)\n\nPoisson equation with right-hand side f, which can be a space-dependent function or a vector. The Poisson equation is defined as\n\n    -Delta u = f\n\nSee also Laplacian.\n\n\n\n\n\n","category":"type"},{"location":"ref/#KernelInterpolation.Semidiscretization","page":"Reference","title":"KernelInterpolation.Semidiscretization","text":"Semidiscretization(spatial_discretization, initial_condition)\nSemidiscretization(equations, nodeset_inner, boundary_condition, nodeset_boundary, [centers,]\n                   initial_condition, kernel = GaussKernel{dim(nodeset_inner)}())\n\nSemidiscretization of a partial differential equation with Dirichlet boundary conditions and initial condition initial_condition. The boundary_condition function can be time- and space-dependent. The initial_condition function is time- and space-dependent to be able to reuse it as analytical solution if available. If no analytical solution is available, the time variable can be ignored in the initial_condition function. The centers are the centers of the kernel functions. By default, centers is set to merge(nodeset_inner, nodeset_boundary). Note that centers needs to have the center number of nodes as the number of nodes in the domain and on the boundary because OrdinaryDiffEq.jl does not support DAEs with rectangular mass matrices.\n\nSee also SpatialDiscretization, semidiscretize.\n\n\n\n\n\n","category":"type"},{"location":"ref/#KernelInterpolation.SpatialDiscretization","page":"Reference","title":"KernelInterpolation.SpatialDiscretization","text":"SpatialDiscretization(equations, nodeset_inner, boundary_condition, nodeset_boundary, basis)\nSpatialDiscretization(equations, nodeset_inner, boundary_condition, nodeset_boundary,\n                      [centers,] kernel = GaussKernel{dim(nodeset_inner)}())\n\nSpatial discretization of a partial differential equation with Dirichlet boundary conditions. The nodeset_inner are the nodes in the domain and nodeset_boundary are the nodes on the boundary. The boundary_condition is a function describing the Dirichlet boundary conditions. The centers are the centers of the kernel functions. By default, centers is set to merge(nodeset_inner, nodeset_boundary). Otherwise, a least squares problem is solved.\n\nSee also Semidiscretization, solve_stationary.\n\n\n\n\n\n","category":"type"},{"location":"ref/#KernelInterpolation.semidiscretize-Tuple{Semidiscretization, Any}","page":"Reference","title":"KernelInterpolation.semidiscretize","text":"semidiscetize(semi::Semidiscretization, tspan)\n\nWrap a Semidiscretization object into an ODEProblem object with time span tspan.\n\n\n\n\n\n","category":"method"},{"location":"ref/#KernelInterpolation.solve_stationary-Union{Tuple{SpatialDiscretization{Dim, RealT}}, Tuple{RealT}, Tuple{Dim}} where {Dim, RealT}","page":"Reference","title":"KernelInterpolation.solve_stationary","text":"solve_stationary(spatial_discretization)\n\nSolve a stationary partial differential equation discretized as spatial_discretization with Dirichlet boundary conditions by non-symmetric collocation (Kansa method). Returns an Interpolation object.\n\n\n\n\n\n","category":"method"},{"location":"ref/#KernelInterpolation.interpolation_matrix","page":"Reference","title":"KernelInterpolation.interpolation_matrix","text":"interpolation_matrix(centers, kernel, ps, regularization = NoRegularization(); factorization_method = Symmetric)\ninterpolation_matrix(basis, ps, regularization; factorization_method = Symmetric)\n\nReturn the interpolation matrix for the basis, polynomials ps, and regularization. For the StandardBasis, the interpolation matrix is defined as\n\n    A = beginpmatrixK  PP^T  0endpmatrix\n\nwhere K is the regularize!d kernel_matrix and P the polynomial_matrix. If a node set of centers and a kernel are given, the interpolation matrix is computed for the StandardBasis. Additionally, you can specify a factorization_method to use for the system matrix. By default, the system matrix is just wrapped as a Symmetric matrix, but you can, e.g., also explicitly use cholesky, lu, or qr factorization.\n\n\n\n\n\n","category":"function"},{"location":"ref/#KernelInterpolation.kernel_matrix","page":"Reference","title":"KernelInterpolation.kernel_matrix","text":"kernel_matrix(basis, nodeset = centers(basis))\nkernel_matrix(nodeset1[, nodeset2], kernel)\n\nReturn the kernel matrix for the nodes and kernel. The kernel matrix is defined as\n\n    A_ij = b_j(x_i)\n\nwhere b_i are the basis function in the basis and x_i are the nodes in the nodeset. If two nodesets and a kernel are given, the kernel matrix is computed for the StandardBasis meaning\n\n    A_ij = K(xi_j x_i)\n\nwhere xi_j are the nodes/centers in nodeset1, x_i are the nodes in nodeset2, and K is the kernel. If nodeset2 is not given, it defaults to nodeset1.\n\n\n\n\n\n","category":"function"},{"location":"ref/#KernelInterpolation.least_squares_matrix","page":"Reference","title":"KernelInterpolation.least_squares_matrix","text":"least_squares_matrix(basis, nodeset, ps, regularization = NoRegularization();\n                     factorization_method = Matrix)\nleast_squares_matrix(centers, nodeset, kernel, ps, regularization = NoRegularization();\n                     factorization_method = Matrix)\n\nReturn the least squares matrix for the basis, nodeset, polynomials ps, and regularization. For the StandardBasis, the least squares matrix is defined as\n\n    A = beginpmatrixK  P_1P_2^T  0endpmatrix\n\nwhere K is the regularize!d kernel_matrix, P_1 the polynomial_matrix for the nodeset and P_2 the polynomial_matrixfor thecenters. If anodesetandkernelare given, the least squares matrix is computed for the [StandardBasis](@ref). Additionally, you can specify afactorization_methodto use for the system matrix. By default, the system matrix is not factorized, but you can, e.g., also explicitly use theqr` factorization.\n\n\n\n\n\n","category":"function"},{"location":"ref/#KernelInterpolation.operator_matrix-NTuple{4, Any}","page":"Reference","title":"KernelInterpolation.operator_matrix","text":"operator_matrix(diff_op_or_pde, nodeset_inner, nodeset_boundary, kernel)\n\nCompute the operator matrix L discretizing mathcalL for a given kernel. The operator matrix is defined as\n\n    L = A_mathcalL A^-1\n\nwhere A_mathcalL is the matrix of the differential operator (defined by the equations), and A the kernel matrix.\n\nSee also pde_boundary_matrix and kernel_matrix.\n\n\n\n\n\n","category":"method"},{"location":"ref/#KernelInterpolation.pde_boundary_matrix-NTuple{5, Any}","page":"Reference","title":"KernelInterpolation.pde_boundary_matrix","text":"pde_boundary_matrix(diff_op_or_pde, nodeset_inner, nodeset_boundary, [centers,] kernel)\n\nCompute the matrix of a partial differential equation (or differential operator) with a given kernel. The matrix is defined as\n\n    A_mathcalL = beginpmatrixtilde A_mathcalLtilde Aendpmatrix\n\nwhere tilde A_mathcalL is the matrix of the differential operator (defined by the equations) for the inner nodes x_i:\n\n    (tilde A_mathcalL)_ij = mathcalLK(x_i xi_j)\n\nand tilde A is the kernel matrix for the boundary nodes:\n\n    tilde A_ij = K(x_i xi_j)\n\nwhere mathcalL is the differential operator (defined by the equations), K the kernel, x_i are the nodes in nodeset_boundary and xi_j are the centers. By default, centers is set to merge(nodeset_inner, nodeset_boundary).\n\nSee also pde_matrix and kernel_matrix.\n\n\n\n\n\n","category":"method"},{"location":"ref/#KernelInterpolation.pde_matrix-NTuple{4, Any}","page":"Reference","title":"KernelInterpolation.pde_matrix","text":"pde_matrix(diff_op_or_pde, nodeset1, nodeset2, kernel)\n\nCompute the matrix of a partial differential equation (or differential operator) with a given kernel. The matrix is defined as\n\n    (tilde A_mathcalL)_ij = mathcalLK(x_i xi_j)\n\nwhere mathcalL is the differential operator (defined by the equations), K the kernel, x_i are the nodes in nodeset1 and xi_j are the nodes in nodeset2.\n\n\n\n\n\n","category":"method"},{"location":"ref/#KernelInterpolation.polynomial_matrix-Tuple{NodeSet, Any}","page":"Reference","title":"KernelInterpolation.polynomial_matrix","text":"polynomial_matrix(nodeset, ps)\n\nReturn the polynomial matrix for the nodeset and polynomials. The polynomial matrix is defined as\n\n    A_ij = p_j(x_i)\n\nwhere x_i are the nodes in the nodeset and p_j the polynomials.\n\n\n\n\n\n","category":"method"},{"location":"ref/#KernelInterpolation.AliveCallback","page":"Reference","title":"KernelInterpolation.AliveCallback","text":"AliveCallback(io::IO = stdout; interval::Integer=0, dt=nothing)\n\nInexpensive callback showing that a simulation is still running by printing some information such as the current time to the screen every interval time steps or after a time of dt in terms of integration time by adding additional (shortened) time steps where necessary (note that this may change the solution).\n\n\n\n\n\n","category":"type"},{"location":"ref/#KernelInterpolation.SummaryCallback","page":"Reference","title":"KernelInterpolation.SummaryCallback","text":"SummaryCallback(io::IO = stdout)\n\nCreate and return a callback that resets the timer at the beginning of a simulation and prints the timer values at the end of the simulation.\n\n\n\n\n\n","category":"type"},{"location":"ref/#KernelInterpolation.SaveSolutionCallback","page":"Reference","title":"KernelInterpolation.SaveSolutionCallback","text":"SaveSolutionCallback(; interval::Integer=0,\n                     dt=nothing,\n                     save_initial_solution=true,\n                     save_final_solution=true,\n                     output_directory=\"out\",\n                     extra_functions=(),\n                     keys=append!([\"itp\"], \"value_\" .* string.(eachindex(extra_functions))))\n\nSave the current numerical solution in regular intervals in VTK format as a Paraview Collection (.pvd). Either pass interval to save every interval time steps or pass dt to save in intervals of dt in terms of integration time by adding additional (shortened) time steps where necessary (note that this may change the solution). The interpolation object will always be saved at the inner and boundary nodes of the corresponding Semidiscretization. You can pass extra functions (time- and space-dependent) or vectors to save at these nodes via extra_functions. The corresponding keys in the .vtu files can be specified by keys.\n\nSee also add_to_pvd, vtk_save.\n\n\n\n\n\n","category":"type"},{"location":"ref/#KernelInterpolation.add_to_pvd-Tuple{Any, Any, Any, NodeSet, Vararg{Any}}","page":"Reference","title":"KernelInterpolation.add_to_pvd","text":"add_to_pvd(filename, pvd, time, nodeset::NodeSet, functions_or_vectors...;\n           keys = \"value_\" .* string.(eachindex(functions_or_vectors)))\n\nSame as vtk_save, but appends the data to a Paraview collection file pvd at time time. In contrast to vtk_save, the functions are time- and space-dependent.\n\n\n\n\n\n","category":"method"},{"location":"ref/#KernelInterpolation.vtk_read-Tuple{Any}","page":"Reference","title":"KernelInterpolation.vtk_read","text":"vtk_read(filename)\n\nRead a set of nodes from a VTK file and return them as a NodeSet. Note that the data will always be returned as a 3D NodeSet, even if the data is 1D or 2D. The point data will be returned as a dictionary with the keys being the names of the data arrays in the VTK file.\n\nSee also vtk_save.\n\n\n\n\n\n","category":"method"},{"location":"ref/#KernelInterpolation.vtk_save-Tuple{Any, NodeSet, Vararg{Any}}","page":"Reference","title":"KernelInterpolation.vtk_save","text":"vtk_save(filename, nodeset::NodeSet, functions_or_vectors...;\n         keys = \"value_\" .* string.(eachindex(functions_or_vectors)))\n\nSave a NodeSet to a VTK file. You can optionally pass a list of space-dependent functions or vectors to save the values of the functions at the nodes. The functions can also be passed as Interpolation or directly as vectors. The optional keyword argument keys is used to specify the names of the data arrays in the VTK file.\n\nSee also add_to_pvd, vtk_read.\n\n\n\n\n\n","category":"method"},{"location":"ref/#KernelInterpolation.default_example-Tuple{}","page":"Reference","title":"KernelInterpolation.default_example","text":"default_example()\n\nReturn the path to an example that can be used to quickly see KernelInterpolation.jl in action. See also examples_dir and get_examples.\n\n\n\n\n\n","category":"method"},{"location":"ref/#KernelInterpolation.examples_dir-Tuple{}","page":"Reference","title":"KernelInterpolation.examples_dir","text":"examples_dir()\n\nReturn the directory where the example files provided with KernelInterpolation.jl are located.\n\nExamples\n\nreaddir(examples_dir())\n\n\n\n\n\n","category":"method"},{"location":"ref/#KernelInterpolation.get_examples-Tuple{}","page":"Reference","title":"KernelInterpolation.get_examples","text":"get_examples()\n\nReturn a list of all examples that are provided by KernelInterpolation.jl. See also examples_dir and default_example.\n\n\n\n\n\n","category":"method"},{"location":"#KernelInterpolation.jl","page":"Home","title":"KernelInterpolation.jl","text":"(Image: Docs-stable) (Image: Docs-dev) (Image: Build Status) (Image: codecov) (Image: Coveralls) (Image: Aqua QA) (Image: License: MIT) (Image: DOI)\n\nKernelInterpolation.jl is a Julia package that implements methods for multivariate interpolation in arbitrary dimension based on symmetric (conditionally) positive-definite kernels with a focus on radial basis functions. It can be used for classical interpolation of scattered data, as well as for generalized (Hermite-Birkhoff) interpolation by using a meshfree collocation approach. This can be used to solve partial differential equations both stationary ones and time-dependent ones by using some time integration method from OrdinaryDiffEq.jl.","category":"section"},{"location":"#Installation","page":"Home","title":"Installation","text":"If you have not yet installed Julia, then you first need to download Julia. Please follow the instructions for your operating system. KernelInterpolation.jl works with Julia v1.10 and newer. KernelInterpolation.jl is a registered Julia package. Therefore, you can install it by executing the following commands from the Julia REPL\n\njulia> using Pkg\n\njulia> Pkg.add(\"KernelInterpolation\")\n\nFor visualizing the results, additionally you need to install Plots.jl, which can be done by\n\njulia> using Pkg\n\njulia> Pkg.add(\"Plots\")\n\nTo create special node sets, you might also want to install QuasiMonteCarlo.jl or Meshes.jl and for solving time-dependent partial differential equations OrdinaryDiffEq.jl in a similar way as above for Plots.jl. Consider using the subpackage OrdinaryDiffEqRosenbrock.jl as it contains the most relevant time integration schemes for this package. See the documentation for more examples on how to use these packages in combination with KernelInterpolation.jl.","category":"section"},{"location":"#Usage","page":"Home","title":"Usage","text":"In the Julia REPL, first load the package KernelInterpolation.jl\n\njulia> using KernelInterpolation\n\nIn order to interpolate discrete function values of a (potentially) multivariate function f mathbbR^dto mathbbR, we first need a set of nodes X = x_1ldotsx_nsubsetmathbbR^d, where the function values of f are known. In KernelInterpolation.jl we can, e.g., construct a homogeneous grid on a hypercube in 2 dimensions by calling\n\njulia> nodeset = homogeneous_hypercube(5, (-2, -1), (2, 1))\n\nHere, we specified that the hypercube has 5 nodes along each of the 2 dimensions (i.e. in total we have 5^2 = 25 nodes) and that the boundaries of the cube are given by the lower left corner located at (-2 -1) and the upper right corner at (2 1). Similarly, NodeSets can be constructed by the functions random_hypercube, random_hypercube_boundary, homogeneous_hypercube_boundary, random_hypersphere or random_hypersphere_boundary or by directly passing a set of nodes to the constructor of NodeSet. Besides the nodeset, we need the function values at the nodes. Let's say, we want to reconstruct the function f(x) = sin(x_1cdot x_2). Then, we can create the vector of function values by\n\njulia> f(x) = sin(x[1]*x[2])\njulia> ff = f.(nodeset)\n\nFinally, we obtain the Interpolation object by calling interpolate, where we specify the kernel function that is used for the reconstruction. Here, we take a Gaussian phi(r) = exp(-(varepsilon r)^2) with shape parameter varepsilon = 12 as radial-symmetric basis function:\n\njulia> kernel = GaussKernel{dim(nodeset)}(shape_parameter = 0.5)\njulia> itp = interpolate(nodeset, ff, kernel)\n\nIf the kernel is only conditionally positive definite, the interpolant will be augmented by a polynomial of the corresponding order of the kernel. Another order can also be passed explicitly with the keyword argument m of interpolate. The result itp is an object that is callable on any point xinmathbbR^d, e.g.,\n\njulia> itp([-1.3, 0.26])\n-0.34096946394940986\n\njulia> f([-1.3, 0.26])\n-0.33160091709280176\n\nFor more sophisticated examples also involving solving stationary or time-dependent partial differential equations, see the documentation. More examples can be found in the examples/ subdirectory.","category":"section"},{"location":"#Visualization","page":"Home","title":"Visualization","text":"In order to visualize the results, you need to have Plots.jl installed and loaded\n\njulia> using Plots\n\nA NodeSet can simply be plotted by calling\n\njulia> plot(nodeset)\n\nAn Interpolation object can be plotted by providing a NodeSet at which the interpolation is evaluated. Continuing the example from above, we can visualize the resulting interpolant on a finer grid\n\njulia> nodeset_fine = homogeneous_hypercube(20, (-2, -1), (2, 1))\njulia> plot(nodeset_fine, itp)\n\nTo visualize the true solution f in the same plot as a surface plot we can call\n\njulia> plot!(nodeset_fine, f, st = :surface)\n\nKernelInterpolation.jl also supports exporting (and importing) VTK files, which can be visualized using tools such as ParaView or VisIt. See the documentation for more details.","category":"section"},{"location":"#Referencing","page":"Home","title":"Referencing","text":"You can directly refer to KernelInterpolation.jl as\n\n@misc{lampert2024kernel,\n  title={{K}ernel{I}nterpolation.jl: {M}ultivariate (generalized) scattered data interpolation\n         with symmetric (conditionally) positive definite kernel functions in arbitrary dimension},\n  author={Lampert, Joshua},\n  year={2024},\n  month={06},\n  howpublished={\\url{https://github.com/JoshuaLampert/KernelInterpolation.jl}},\n  doi={10.5281/zenodo.12599880}\n}","category":"section"},{"location":"#Authors","page":"Home","title":"Authors","text":"The package is developed and maintained by Joshua Lampert (University of Hamburg).","category":"section"},{"location":"#License-and-contributing","page":"Home","title":"License and contributing","text":"KernelInterpolation.jl is published under the MIT license (see License). We are pleased to accept contributions from everyone, preferably in the form of a PR.","category":"section"},{"location":"pdes/#Solving-PDEs-by-collocation","page":"Solving PDEs by collocation","title":"Solving PDEs by collocation","text":"Kernel methods are also suitable to solve partial differential equations (PDEs), which is also sometimes known as Hermite-Birkhoff interpolation, a special case of generalized interpolation. In an abstract setting generalized interpolation deals with the following problem: Given a Hilbert space H and a set of functionals lambda_i_i = 1^Nsubset H^* (H^* being the dual space), find a function sin H such that lambda_i(s) = f_i for i = 1ldotsN for given function values f_i. Classical interpolation, discussed in the previous section, corresponds to the case where H is a reproducing kernel Hilbert space (RKHS) and lambda_i(s) = s(x_i) are point evaluations at the nodes x_i for X = x_i_i = 1^N. In the case of Hermite-Birkhoff interpolation, the functionals lambda_i are not point evaluations but differential operators, which are applied to the function s and then evaluated at the nodes x_i.\n\nThe following is mostly based on the book by Fasshauer [Fasshauer2015].","category":"section"},{"location":"pdes/#Stationary-PDEs","page":"Solving PDEs by collocation","title":"Stationary PDEs","text":"Consider the following general stationary PDE in a domain OmegasubsetmathbbR^d:\n\nmathcalLu = f\n\nwhere mathcalL is a linear differential operator of order m, u is the unknown function and f is a given function. The operator mathcalL can be written as\n\nmathcalLu = sum_alphaleq m a_alpha D^alpha u\n\nwhere D^alpha = partial_x_1^alpha_1cdotspartial_x_d^alpha_d is a partial derivative of order alpha = alpha_1 + cdots + alpha_d. Note that in the context of PDEs we usually use the notation u for the unknown function instead of s as in the general interpolation problem. For a complete description of the PDE, we also need to specify boundary conditions on the boundary partialOmega, which can be written with a boundary operator mathcalB as\n\nmathcalBu = g\n\nwhere g is a given function. As boundary operator mathcalB we usually consider the identity operator, which corresponds to Dirichlet boundary conditions. Like in the case of classical interpolation, we pick a set of nodes X_I = x_i_i = 1^N_IsubsetOmega. Due to the additional boundary conditions, we also pick a set of nodes X_B = x_i_i = N_I + 1^NsubsetpartialOmega. Let N = N_I + N_B and X = X_Icup X_B. We again formulate an ansatz function u as a linear combination of basis functions. In the simplest case, we use the same linear combination (neglecting polynomial augmentation for simplicity), i.e.\n\nu(x) = sum_j = 1^N c_jK(x x_j)\n\nwhere K is the kernel function. This approach is also known as non-symmetric collocation or Kansa's method. By enforcing the conditions mathcalLu(x_i) = f(x_i) for i = 1ldotsN_I and mathcalBu(x_i) = g(x_i) for i = N_I + 1ldotsN we obtain a linear system of equations for the coefficients c_i, which can be written as\n\nbeginpmatrix\ntildeA_L  tildeA_B\nendpmatrix\nc = beginpmatrix\nf_X_I  g_X_B\nendpmatrix\n\nwhere tildeA_LinmathbbR^N_Itimes N and tildeA_BinmathbbR^N_Itimes N are the matrices corresponding to the conditions at the interior and boundary nodes, respectively, i.e.\n\nbeginalign*\n    (tildeA_L)_ij = mathcalLK(x_i x_j) i = 1 ldots N_I j = 1 ldots N \n    (tildeA_B)_ij = mathcalBK(x_i x_j) i = 1 ldots N_B j = 1 ldots N\nendalign*\n\nSince the kernel function is known and differentiable, we can compute the derivatives of K analytically.\n\nnote: Note\nIn KernelInterpolation.jl, the derivatives of the kernel function are computed using automatic differentiation (AD) by using ForwardDiff.jl. This allows for flexibility, simplicity, and easier extension, but it might be slower than computing the derivatives analytically. If you are interested in a more efficient implementation, you can have a look at the test set \"Differential operators\" in the test suite of KernelInterpolation.jl. This test set not only shows how to use analytical derivatives, but also how to define your own differential operators, which can be used to define custom PDEs.\n\nNote, however, that the system matrix A = beginpmatrix tildeA_L  tildeA_B endpmatrix is not invertible in general because it not symmetric anymore as it was the case in the classical interpolation. Thus, this approach is also called non-symmetric collocation.\n\nLet us see how this can be implemented in KernelInterpolation.jl by solving the Poisson equation -Delta u = f in an L-shaped domain. We start by defining the equation (thus the differential operator) and the right-hand side. KernelInterpolation.jl already provides a set of predefined differential operators and equations.\n\nusing KernelInterpolation\n\n# right-hand-side of Poisson equation\nf(x, equations) = 5 / 4 * pi^2 * sinpi(x[1]) * cospi(x[2] / 2)\npde = PoissonEquation(f)\n\n# analytical solution of equation\nu(x, equations) = sinpi(x[1]) * cospi(x[2] / 2)\n\nNext, we define the domain and the boundary of the L-shaped domain. We use a homogeneous grid for the nodes and filter the inner and boundary nodes in two separate NodeSets.\n\nfunction create_L_shape(N)\n    x_min1 = (0.0, 0.0)\n    x_max1 = (1 * pi, 1.0)\n    x_min2 = (1 * pi, 0.0)\n    x_max2 = (2 * pi, 1.0)\n    x_min3 = (0.0, 1.0)\n    x_max3 = (1 * pi, 2.0)\n    nodeset1 = homogeneous_hypercube(N, x_min1, x_max1)\n    nodeset2 = homogeneous_hypercube(N, x_min2, x_max2)\n    nodeset3 = homogeneous_hypercube(N, x_min3, x_max3)\n    nodeset = merge(nodeset1, nodeset2, nodeset3)\n    unique!(nodeset)\n    nodeset_inner = empty_nodeset(2)\n    nodeset_boundary = empty_nodeset(2)\n    for x in nodeset\n        if x[1] == 0.0 || x[2] == 0.0 || x[2] == 2.0 || x[1] == 2.0 * pi || (x[1] == 1.0 * pi && x[2] >= 1.0) || (x[2] == 1.0 && x[1] >= pi)\n            push!(nodeset_boundary, x)\n        else\n            push!(nodeset_inner, x)\n        end\n    end\n    return nodeset_inner, nodeset_boundary\nend\nnodeset_inner, nodeset_boundary = create_L_shape(6)\n\nFinally, we define the boundary condition, the kernel, and collect all necessary information in a SpatialDiscretization, which can be solved by calling the solve_stationary function.\n\n# Dirichlet boundary condition (here taken from analytical solution)\ng(x) = u(x, pde)\n\nkernel = WendlandKernel{2}(3, shape_parameter = 0.3)\nsd = SpatialDiscretization(pde, nodeset_inner, g, nodeset_boundary, kernel)\nitp = solve_stationary(sd)\n\nThe result itp is an Interpolation object, which can be used to evaluate the solution at arbitrary points. We can save the solution on a finer grid to a VTK file and visualize it.\n\nmany_nodes_inner, many_nodes_boundary = create_L_shape(20)\nmany_nodes = merge(many_nodes_inner, many_nodes_boundary)\nOUT = \"out\"\nispath(OUT) || mkpath(OUT)\nvtk_save(joinpath(OUT, \"poisson_2d_L_shape\"), many_nodes, itp, x -> u(x, pde);\n         keys = [\"numerical\", \"analytical\"])\nrm(joinpath(OUT, \"poisson_2d_L_shape.vtu\")) #clean up again # hide\n\nThe resulting VTK file can be visualized with a tool like ParaView. After applying the filter Warp by Scalar, setting the coloring accordingly, and changing the \"Representation\" to \"Point Gaussian\", we obtain the following visualization:\n\n(Image: Poisson equation in an L shape domain)","category":"section"},{"location":"pdes/#Time-dependent-PDEs","page":"Solving PDEs by collocation","title":"Time-dependent PDEs","text":"KernelInterpolation.jl also supports the solution of time-dependent PDEs. The idea is to use the same approach as above for the spatial part of the PDE and then obtain an ordinary differential equation (ODE), which can be solved by some time integration method (method of lines). The general form of a time-dependent PDE is\n\npartial_t u + mathcalLu = f\n\nwhere mathcalL is a linear differential operator of order m and f is a given function. The initial condition is given by u(x 0) = u_0(x). Boundary conditions are applied as before. Similar to the stationary case, we discretize the spatial part of the PDE by collocation and obtain a system of ODEs for the coefficients c_j of the basis functions, i.e. now the coefficients depend on time:\n\nu(t x) = sum_j = 1^N c_j(t)K(x x_j)\n\nwhere c_j(t) are the coefficients at time t. We again divide the spatial domain in a set of points in the inner domain X_I and at the boundary X_B. Plugging in the ansatz function into the PDE and the boundary conditions and evaluating at the nodes, we obtain the following system of ODEs and algebraic equations:\n\nbeginalign*\n    partial_t u(t x_i) = sum_j = 1^N partial_t c_j(t)K(x_i x_j) = -sum_j = 1^Nc_j(t)mathcalLK(x_i x_j) + f(x_i) i = 1 ldots N_I \n    0 = -sum_j = 1^Nc_j(t)mathcalBK(x_i x_j) + g(x_i) i = N_I + 1 ldots N\nendalign*\n\nThese equations can be written compactly as a differential-algebraic equation (DAE) of the form\n\nMc^prime(t) = -Ac(t) + b\n\nwhere c(t) = c_1(t) ldots c_N(t)^T is the vector of coefficients, MinmathbbR^Ntimes N is a (singular) mass matrix, AinmathbbR^Ntimes N is the system matrix, and b = beginpmatrixf_X_I g_X_BendpmatrixinmathbbR^N. The matrices are given by\n\nM = beginpmatrix tildeA_I0endpmatrix quadtextandquad A = beginpmatrix tildeA_LtildeA_Bendpmatrix\n\nwhere\n\nbeginalign*\n    tildeA_IinmathbbR^N_Itimes N (tildeA_I)_ij = K(x_i x_j)\n    tildeA_LinmathbbR^N_Itimes N (tildeA_L)_ij = mathcalLK(x_i x_j)\n    tildeA_BinmathbbR^N_Btimes N (tildeA_B)_ij = mathcalBK(x_i x_j)\nendalign*\n\nThe coefficients for the initial conditions can be computed from the initial condition u_0(x) by solving the linear system\n\nbeginpmatrix\ntildeA_ItildeA_B\nendpmatrix\nc_0 = (u_0)_X\n\nFor the solution of the DAE system, KernelInterpolation.jl uses the library OrdinaryDiffEq.jl, which already provides a wide range of time integration methods. Note that this is a differential-algebraic equation (DAE) system, which is more difficult to solve than a simple ODE system. Thus, we are restricted to specialized time integration methods, which can handle DAEs. We recommend using the Rodas5P method, which is a Rosenbrock method for stiff DAEs. See also the documentation of OrdinaryDiffEq.jl for more information. Rodas5P along with other Rosenbrock methods are implemented in the subpackage OrdinaryDiffEqRosenbrock.jl. Therefore, we recommend using this package in combination with KernelInterpolation.jl to reduce precompile time. In order to solve DAEs with OrdinaryDiffEqRosenbrock.jl, we also need to import the subpackage OrdinaryDiffEqNonlinearSolve.jl.\n\nIn KernelInterpolation.jl, you can use the constructor of a Semidiscretization in a very similar way as SpatialDiscretization, but with the additional initial condition. This can be turned into an ODEProblem object from the OrdinaryDiffEq.jl ecosystem by calling semidiscretize. The resulting ODEProblem can then be solved by calling the solve function from OrdinaryDiffEq.jl. The resulting object is an ODESolution object, which can be transferred to a TemporalInterpolation by calling TemporalInterpolation on it. This object acts similarly to an Interpolation, but has an additional time argument, e.g., itp = titp(1.0) gives an interpolation object itp from a temporal interpolation titp at time t = 1.0.\n\nFor a complete example of a time-dependent PDE, see, e.g., an example of the heat equation. KernelInterpolation.jl provides some callbacks that are can be passed to solve in order to call them during the time integration process. These can be used to monitor the solution or to save it to a file. To date, these are AliveCallback, SaveSolutionCallback, and SummaryCallback.","category":"section"},{"location":"pdes/#References","page":"Solving PDEs by collocation","title":"References","text":"[Fasshauer2015]: Fasshauer (2015): Kernel-based Approximation Methods using MATLAB, World Scientific, DOI: 10.1142/9335.","category":"section"},{"location":"nodesets/#nodesets","page":"Sets of nodes","title":"Sets of nodes","text":"Numerical methods based on kernel functions are usually meshfree, i.e. they do not need not any information of connectivity between the different points (a mesh). Instead, they usually solely use a(n) (unstructured) set of points in space X = x_1 ldots x_NsubsetmathbbR^d with N nodes x_i of any dimension dinmathbbN. These vectors are also often called, e.g., points, nodes, centers, or data sites and sets of nodes are also sometimes called point clouds. Since meshes can sometimes be cumbersome to create and handle especially in higher space dimensions, kernel methods are often convenient and flexible for high-dimensional problems. In KernelInterpolation.jl, sets of points are called NodeSet and can be of any dimension. You can create NodeSets simply by passing a matrix, where the rows are the different points or by passing a Vector of Vectors:\n\nusing KernelInterpolation\nnodes = NodeSet([0.0 0.0\n                 0.0 1.0\n                 1.0 0.0\n                 1.0 1.0])\nnodes2 = NodeSet([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])\n\nOne characterization of how well distributed a set of nodes is, is the so-called separation distance, which is defined by\n\nq_X = frac12minlimits_x_ineq x_jx_i - x_j_2\n\nGeometrically, q_X is the radius of the largest ball that can be placed around every node in X such that no two balls overlap. This quantity depends only on the choice of the nodes and is always computed by KernelInterpolation.jl. It can be accessed by calling\n\nq = separation_distance(nodes)\n\nThe separation distance usually plays a crucial role when estimating the stability properties of kernel methods. Another important geometric property of node sets, often essential for the error analysis of kernel methods, is the so-called fill distance given by\n\nh_X = sup_xinOmegamin_x_jin Xx - x_j_2\n\nwhich can be interpreted as the radius of the largest ball that can be placed in Omega such that the ball does not contain any point in X. This quantity depends on the choice of a domain OmegasubsetmathbbR^d that covers X and can therefore not solely be computed by the NodeSet. However, it can be estimated by creating a fine grid of points inside Omega. Let's say we take Omega = 01^2. We can conveniently create a set of equidistant points within any hypercube by calling homogeneous_hypercube:\n\nnodes_fine = homogeneous_hypercube(20, (0.0, 0.0), (1.0, 1.0))\n\nThis creates a NodeSet with 20 nodes equally spaced along both dimensions. The distance matrix of the two sets, i.e. the matrix with entries D_ij = x_i - xi_j_2 for x_iin X and xi_j being the evaluation points in Omega, can be obtained by calling the function distance_matrix:\n\nD = distance_matrix(nodes, nodes_fine)\n\nFinally, the fill distance is approximated by\n\nh = maximum(minimum(D, dims = 1))\n\nNote that this is only an estimate. The true fill distance is sqrt22approx 0707 (and reached by placing xinOmega at (05 05)^T). The estimate can be improved by taking a finer evaluation grid.\n\nNext to homogeneous_hypercube, KernelInterpolation.jl provides additional convenience functions to create specific commonly used NodeSets. These are homogeneous_hypercube_boundary to create equally spaced nodes at the boundary of a hypercube, random_hypercube and random_hypercube_boundary to create random uniformly distributed nodes inside or at the boundary of a hypercube, and random_hypersphere and random_hypersphere_boundary for random uniformly distributed nodes inside or at the boundary of a hypersphere. Note that the first argument n of the homogeneous_* functions denotes the number of points along each dimension, while for the random_* function it denotes the number of total generated nodes.\n\nOther sampling methods for hypercubes of any dimension can be obtained by using the package QuasiMonteCarlo.jl. We can simply pass the (transposed) result of any sampling algorithm from QuasiMonteCarlo.jl to the constructor of NodeSet. To create, e.g., 500 Halton points in a box bounded by -10 -10 -10 and 20 20 20 we can run:\n\nusing QuasiMonteCarlo: sample, HaltonSample\nnodes_matrix = sample(500, [-1.0, -1.0, -1.0], [2.0, 2.0, 2.0], HaltonSample())\nnodes_halton = NodeSet(nodes_matrix')\n\nFor the available sampling algorithms in QuasiMonteCarlo.jl, see the overview in the documentation.\n\nAnother possibility to create more advanced NodeSets is by using the package Meshes.jl and the sampling algorithms defined therein. For example, we can create a regularly sampled set of nodes on the surface of a sphere by running:\n\nusing Meshes: Meshes, Sphere, Point, RegularSampling\nsphere = Sphere(Point(0.0, 0.0, 0.0), 1.0)\nsampler = RegularSampling(20, 30)\npoints = Meshes.sample(sphere, sampler)\nnodes = NodeSet(collect(points))\n\nFor more information on the available sampling algorithms in Meshes.jl, see the documentation. In the documentation of Meshes.jl, you can also find information on how to create more complex geometries like ellipsoids, tori, and many more. In general, a PointSet from Meshes.jl or a Vector of Points can be directly passed to the constructor of a NodeSet and vice versa can a NodeSet be passed to the constructor of a PointSet.\n\nMore complicated NodeSets consisting of different shapes can be created, e.g., by mergeing different NodeSets.","category":"section"},{"location":"nodesets/#Visualizing-[NodeSet](@ref)s","page":"Sets of nodes","title":"Visualizing NodeSets","text":"To visualize a NodeSet, there are currently three possibilities. The first one uses Plots.jl. After installing and loading Plots.jl, we can then simply call plot on any 1D, 2D, or 3D NodeSet to plot it.\n\nusing Plots\nplot(nodes_halton)\nsavefig(\"nodes_halton.png\") # hide\nnothing # Avoid showing the path # hide\n\n(Image: Halton nodes)\n\nYou might want to consider using other plotting backends, e.g. PyPlot.jl can be used by additionally calling pyplot() before plot in the above code snippet. Refer to the documentation of Plots.jl for the different available backends. In order to color the nodes according to the values of a function (or an Interpolation) at the nodes, you can additionally pass the vector of function values as keyword argument zcolor (note that you can treat a NodeSet as a usual array, e.g., broadcasting works with the common dot syntax).\n\nf(x) = sinpi(x[1])\nplot(nodes_halton, zcolor = f.(nodes_halton))\nsavefig(\"nodes_halton_function.png\") # hide\nnothing # Avoid showing the path # hide\n\n(Image: Halton nodes with function values)\n\nFor 1D or 2D NodeSets you can also pass a function (or, again, an object of Interpolation), which is then used to determine the values in the vertical direction. For a surface plot of a function based on a set of nodes, you can, e.g., run the following\n\nplot(nodes_fine, f, st = :surface)\nsavefig(\"nodes_fine.png\") # hide\nnothing # Avoid showing the path # hide\n\n(Image: Surface plot)\n\nAs an alternative to plotting from within Julia, you can save NodeSets to the commonly used VTK files and then view the result, e.g., in ParaView or VisIt. You can save a NodeSet simply by using vtk_save and passing a filename as well as the NodeSet:\n\nvtk_save(\"nodes_halton\", nodes_halton)\n\nAgain, you can additionally save node values by passing additional functions or vectors (of the same size as the NodeSet), which can also be visualized with ParaView or VisIt. Note that you can also read back in a NodeSet (and possibly the additional node values) by using vtk_read:\n\nnodes_halton2, _ = vtk_read(\"nodes_halton.vtu\")\nrm(\"nodes_halton.vtu\") #clean up again # hide\nall(nodes_halton2 .== nodes_halton)\n\nLastly, you can convert any NodeSet to a PointSet from Meshes.jl by calling PointSet(nodes) and use the plotting capabilities of Meshes.jl, which uses Makie.jl as backend. For more information, we refer to the documentation of Meshes.jl.\n\nusing Meshes: PointSet, viz\nimport CairoMakie\npoints = PointSet(nodes_halton)\nviz(points)\nCairoMakie.save(\"nodes_halton_Makie.png\", CairoMakie.current_figure()) # hide\nnothing # Avoid showing the path # hide\n\n(Image: Makie plot)","category":"section"}]
}
